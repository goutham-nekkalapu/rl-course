{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Multi-arm Bandits Algorithms</h2></center>\n",
    "\n",
    "\n",
    "<center><img src=\"http://i1.wp.com/banditalgs.com/wp-content/uploads/2016/09/cropped-bandit-algorithm-full.png?fit=512%2C512\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "Explain and code the following multi-arm bandits algorithms:\n",
    "\n",
    "1. Random\n",
    "1. Epsilon-Greedy Algorithm  \n",
    "2. Softmax  \n",
    "4. Bayesian Bandit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Random Algorithm</h2></center>\n",
    "\n",
    "1) Pure exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Good for checking the entire system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Reasonable baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Let's Code a Bandit</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "palette = \"Dark2\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import common packages\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Seed the random number generators so you get the same results every time.\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Bandit is the class that setups and runs our multi-arm bandit\n",
    "from bandit import Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def random_choice(self): \n",
    "    \"Pick a bandit uniformly at random.\"\n",
    "    pass\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def random_choice(self): \n",
    "    \"Pick a bandit uniformly at random.\"\n",
    "    return np.random.randint(low=0, high=self.n_bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def random_choice(self): \n",
    "    \"Pick a bandit uniformly at random.\"\n",
    "    return np.random.randint(low=0, high=self.n_bandits) # Return the index of the winning bandit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Why use `np.random.randint` instead of `random.choice`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`random.choice` is much slower because it selects among any options.\n",
    "\n",
    "`np.random.randint` is optimitize to just select integers (the index of the arm in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.6, 0.6, 0.6, 0.6],\n",
    "           choice_function=random_choice)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "b.max_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.60   0.60   0.60   0.60\n",
      "Number of wins:      165    155    132    147\n",
      "Number of trials:    263    258    243    236\n",
      "Conversion rates:   0.63   0.60   0.54   0.62\n",
      "\n",
      "A total of 599 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "from bandit import print_results\n",
    "\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/eg.jpg\" width=\"55%\"/></center>\n",
    "\n",
    "Some percent of the time we explore - randomly choose one of the options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The rest of the time we exploit - choose the option that has so far had the highest conversion rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Epsilon-Greedy Algorithm</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/epislon_greedy.png\" width=\"75%\"/></center>\n",
    "\n",
    "a is action.  \n",
    "t is time.  \n",
    "$a^*$ is optimal action.  \n",
    "ε epsilon a parameter to choose is the probability that we explore.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What is the range for epsilon?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Epsilon is a probability, thus bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Epsilon Default Value</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>.1 (or 10% exploration)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def epsilon_greedy(self, epsilon=0.1):\n",
    "    \"\"\"Pick a bandit uniformly at random epsilon percent of the time.\n",
    "    Otherwise pick the bandit with the best observed proportion of winning.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    pass\n",
    "```               \n",
    "\n",
    "Write the code without looking at other people's implementation.   \n",
    "It is okay to look at course notes and library documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(self, epsilon=0.1):\n",
    "    \"\"\"Pick a bandit uniformly at random epsilon percent of the time.\n",
    "    Otherwise pick the bandit with the best observed proportion of winning.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    if random.random() <= epsilon:\n",
    "        return np.random.randint(low=0, high=self.n_bandits)\n",
    "    else:\n",
    "        return np.argmax(self.wins / (self.trials+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(self, epsilon=0.1):\n",
    "    \"\"\"Pick a bandit uniformly at random epsilon percent of the time.\n",
    "    Otherwise pick the bandit with the best observed proportion of winning.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    if random.random() <= epsilon:\n",
    "        return np.random.randint(low=0, high=self.n_bandits) # Return a random choice\n",
    "    else:\n",
    "        return np.argmax(self.wins / (self.trials + 1)) # Return the current best choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.90   0.60   0.60   0.60\n",
      "Number of wins:      848     11      8      9\n",
      "Number of trials:    936     23     16     25\n",
      "Conversion rates:   0.91   0.48   0.50   0.36\n",
      "\n",
      "A total of 876 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.9, 0.6, 0.6, 0.6],\n",
    "                    choice_function=epsilon_greedy)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>A Common Real-world Design Pattern for Epsilon-Greedy </h2></center>\n",
    "    \n",
    "1. At the beginning, set epsilon high (.15-.25).\n",
    "1. After fix number of trials, lower (.05-.1).\n",
    "1. Keep low and fixed for remaining trails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is similar to a fixed schedule for learning rate decay for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are limitations of Epsilon-Greedy algorithm?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) At the beginning, we might select exploitation but do __not__ yet know which version is right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) If epsilon is .10, the epsilon-greedy algorithm will eventually show the best option 90% of the time. \n",
    "\n",
    "So this means that 10% of the time the algorithm will continue to randomly show different versions of the site, no matter how confident we are that a certain version is the best. There is a point where we should stop the exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Softmax Algorithm</h2></center>\n",
    "\n",
    "Choose the arm in proportion to its estimated value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Say $p_A$, $p_B$ and $p_C$ are the conversion rates for three versions of the site. We would choose site A with the following probability  \n",
    "\n",
    "$$ \\frac{exp(p_A/ \\tau)}{exp(p_A/ \\tau) + exp(p_B/ \\tau) + exp(p_C/ \\tau)} $$\n",
    "\n",
    "$\\tau$ is a chosen parameter that controls the ‘randomness’ of the choice, usually around 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def softmax(self, tau=0.01):\n",
    "    \"\"\"Pick an bandit according to the Boltzman Distribution.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    pass\n",
    "```                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(self, tau=0.01):\n",
    "    \"\"\"Pick an bandit according to the Boltzman Distribution.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "\n",
    "    # Make sure to play each bandit at least once\n",
    "    if len(self.trials.nonzero()[0]) < self.n_bandits:\n",
    "        return np.random.randint(0, self.n_bandits)\n",
    "\n",
    "    # Compute the terms of the softmax function\n",
    "    numerators = np.zeros(len(self.wins))\n",
    "    denom = 0\n",
    "    for i in range(len(self.wins)):\n",
    "        numerators[i] = math.exp((self.wins[i] / (self.trials[i]+1)) / tau)\n",
    "        denom += numerators[i]\n",
    "\n",
    "    # Compute the \"pmf\" of the bandits\n",
    "    last = 0\n",
    "    softmax_vals = np.zeros(len(self.wins))\n",
    "    for i in range(len(self.wins)):\n",
    "        softmax_vals[i] = last + (numerators[i] / denom)\n",
    "        last = softmax_vals[i]\n",
    "\n",
    "    # Pick a random number and see where it falls in the range\n",
    "    r = random.random()\n",
    "    for i in range(len(self.wins)):\n",
    "        if r <= softmax_vals[i]:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.90   0.60   0.60   0.60\n",
      "Number of wins:        1    577      2      0\n",
      "Number of trials:      2    994      3      1\n",
      "Conversion rates:   0.50   0.58   0.67   0.00\n",
      "\n",
      "A total of 580 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.9, 0.6, 0.6, 0.6],\n",
    "                    choice_function=softmax)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Softmax Limitation</h2></center>\n",
    "\n",
    "It does __not__ take into account how many times an arm has been pulled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayesian Bandit (or Thompson Sampling)</h2></center>\n",
    "\n",
    "1. Calculate Posterior for each \"arm\" / action\n",
    "2. Pick a random sample from each Posterior\n",
    "3. Choose the arm with highest reward\n",
    "4. Update Posteriors for next round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayesian Bandit (or Thompson Sampling)</h2></center>\n",
    "\n",
    "<br>\n",
    "<center><img src=\"http://fastml.com/images/ab-testing/bandits_small.png\" width=\"50%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<center><a href=\"https://learnforeverlearn.com/bandits/\">Demo!</a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Thompson Sampling Steps</h2></center>\n",
    "\n",
    "1. For each k, sample $θ_k ∼ π(θ_k | Dt)$\n",
    "1. Then play bandit $k = arg max_k \\hatθ_k$ \n",
    "1. Update played bandit win or lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Thompson Sampling for Bernoulli Bandits</h2></center>\n",
    "\n",
    "Given k bandits, each k pays off a reward of 1 with probability\n",
    "$θ_k$ and 0 with probability 1 − $θ_k$. \n",
    "\n",
    "Model each bandit as $π$. Parameterized as $π(θ_k | D_t)$\n",
    "\n",
    "Parameterization of the Beta family of distribution $θ ∼ Beta(α, β)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Simple (but effective) Posterior is:\n",
    "\n",
    "$$θ_k | D_t ∼ Beta(1 + n_{k0}, 1 + n_{k1})$$\n",
    "\n",
    "\n",
    "Assume a uniform distribution is achieved using prior α = prior β = 1. \n",
    "\n",
    "- $\\alpha = 1 + $ number of times bandit has won\n",
    "- $\\beta = 1 + $ number of times bandit has lost  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Another sensible defaults is to use an uninformative prior by setting prior_alpha = prior_beta = 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Full Bayesian updating</h2></center>\n",
    "\n",
    "- Useful if you do not assume have stationary data\n",
    "- However, it is far more complicated and much slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Use the Bayesian Beta-Binomial conjugate prior techniques used to model the click-through rate as our base model for each of the bandits.  \n",
    "\n",
    "Sample from each bandit’s distribution and play the bandit with the highest value.\n",
    "\n",
    "It will naturally converge on the bandit with the best payout.  \n",
    "\n",
    "[Source](http://fastml.com/ab-testing-with-bayesian-bandits-in-google-analytics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://davidrosenberg.github.io/mlcourse/in-prep/thompson-sampling-bernoulli.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def bayesian_bandit(self):\n",
    "    \"\"\"Randomly sample from a beta distribution for each bandit and pick the one\n",
    "    with the largest value.\n",
    "    Return the index of the winning bandit.\"\"\"    \n",
    "    pass\n",
    "```     \n",
    "\n",
    "Hints: \n",
    "\n",
    "- Use `scipy.stats.beta`\n",
    "- Seriously - use the all the methods on `scipy.stats.beta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_bandit(self):\n",
    "    \"\"\"Randomly sample from a beta distribution for each bandit and pick the one\n",
    "    with the largest value.\n",
    "    Return the index of the winning bandit.\"\"\"    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_bandit(self):\n",
    "    \"\"\"Randomly sample from a beta distribution for each bandit and pick the one\n",
    "    with the largest value.\n",
    "    Return the index of the winning bandit.\"\"\"    \n",
    "    \n",
    "    # Make sure to play each bandit at least once\n",
    "    if len(self.trials.nonzero()[0]) < self.n_bandits:\n",
    "        return np.random.randint(0, self.n_bandits)\n",
    "\n",
    "    # Create a beta distribution for each bandit and draw a random sample\n",
    "    samples = np.zeros(len(self.wins))\n",
    "    for i in range(len(self.wins)):\n",
    "        samples[i] = stats.beta(1+self.wins[i], (1+self.trials[i]-self.wins[i])).rvs(1)\n",
    "    return np.argmax(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.90   0.60   0.60   0.60\n",
      "Number of wins:      872      4      5     11\n",
      "Number of trials:    966      8      9     17\n",
      "Conversion rates:   0.90   0.50   0.56   0.65\n",
      "\n",
      "A total of 892 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.9, 0.6, 0.6, 0.6],\n",
    "                    choice_function=bayesian_bandit)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Simulate Results With The Implementations</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.00   0.00   0.00   0.00\n",
      "Number of wins:        0      0      0      0\n",
      "Number of trials:    926     28     25     21\n",
      "Conversion rates:   0.00   0.00   0.00   0.00\n",
      "\n",
      "A total of 0 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.00   0.00   0.00   0.00\n",
      "Number of wins:        0      0      0      0\n",
      "Number of trials:    241    247    271    241\n",
      "Conversion rates:   0.00   0.00   0.00   0.00\n",
      "\n",
      "A total of 0 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.00   0.00   0.00   0.00\n",
      "Number of wins:        0      0      0      0\n",
      "Number of trials:    246    249    250    255\n",
      "Conversion rates:   0.00   0.00   0.00   0.00\n",
      "\n",
      "A total of 0 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "p_array = [0]*4 # No winners\n",
    "# p_array = [1]*4 # All winners\n",
    "# p_array = [.5]*4 # All the same\n",
    "# p_array = [.01, .4, .8, .999] # All different\n",
    "\n",
    "strategies = [epsilon_greedy, softmax, bayesian_bandit]\n",
    "\n",
    "for strategy in strategies:\n",
    "    b = Bandit(p_array=p_array,\n",
    "                choice_function=strategy)\n",
    "    b.sample_bandits(n=1_000) # 1000 is a good value\n",
    "    print()\n",
    "    print('#'*50)\n",
    "    print()\n",
    "    print(strategy.__name__.title())\n",
    "    print_results(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.90\n",
      "Number of wins:        7      2      3    743\n",
      "Number of trials:    107     29     22    842\n",
      "Conversion rates:   0.07   0.07   0.14   0.88\n",
      "\n",
      "A total of 755 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.90\n",
      "Number of wins:        0      0      0    881\n",
      "Number of trials:      3      1      1    995\n",
      "Conversion rates:   0.00   0.00   0.00   0.89\n",
      "\n",
      "A total of 881 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.90\n",
      "Number of wins:        0      0      0    882\n",
      "Number of trials:      3      3      3    991\n",
      "Conversion rates:   0.00   0.00   0.00   0.89\n",
      "\n",
      "A total of 882 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.12\n",
      "Number of wins:        7     87      4      2\n",
      "Number of trials:    110    822     42     26\n",
      "Conversion rates:   0.06   0.11   0.10   0.08\n",
      "\n",
      "A total of 100 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.12\n",
      "Number of wins:        0     87      0      0\n",
      "Number of trials:      4    991      4      1\n",
      "Conversion rates:   0.00   0.09   0.00   0.00\n",
      "\n",
      "A total of 87 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.12\n",
      "Number of wins:       22     17     38      9\n",
      "Number of trials:    270    179    399    152\n",
      "Conversion rates:   0.08   0.09   0.10   0.06\n",
      "\n",
      "A total of 86 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.20   0.40   0.50\n",
      "Number of wins:        2      5     13    425\n",
      "Number of trials:     26     28     31    915\n",
      "Conversion rates:   0.08   0.18   0.42   0.46\n",
      "\n",
      "A total of 445 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.20   0.40   0.50\n",
      "Number of wins:        0      0      0    477\n",
      "Number of trials:      1      2      1    996\n",
      "Conversion rates:   0.00   0.00   0.00   0.48\n",
      "\n",
      "A total of 477 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.20   0.40   0.50\n",
      "Number of wins:        2     17     12    445\n",
      "Number of trials:     15     52     39    894\n",
      "Conversion rates:   0.13   0.33   0.31   0.50\n",
      "\n",
      "A total of 476 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "probs ={'atlantic_city': [0.1, 0.1, 0.1, 0.90],\n",
    "        'las_vegas':     [0.1, 0.1, 0.1, 0.12],\n",
    "        'reno':          [0.1, 0.2, 0.4, 0.50]}\n",
    "\n",
    "for prob in  probs.items():\n",
    "    p_array = prob[1]\n",
    "    for strategy in strategies:\n",
    "        b = Bandit(p_array=p_array,\n",
    "                    choice_function=strategy)\n",
    "        b.sample_bandits(n=1_000) # 1000 is a good value\n",
    "        print()\n",
    "        print('#'*50)\n",
    "        print()\n",
    "        print(strategy.__name__.title())\n",
    "        print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Which MAB strategy should I use?</h2></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bandit_choice.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/map.jpeg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Challenge Question</h2></center>\n",
    "\n",
    "Think about putting this into production. What kind of testing would you have to do (unit, integration, acceptance)? How would you monitor the different choices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Epsilon-Greedy Algorithm sometimes pick randomly, other times pick the best.\n",
    "- Softmax is structured exploration.\n",
    "- Bayesian Bandit Algorithm models what we have seen (and uncertainty). Randomly sample, then pick best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bonus Materials</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which strategy should I use?\n",
    "------\n",
    "<center><img src=\"images/compare.png\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3) Upper Confidence Bound (UCB)\n",
    "-----\n",
    "\n",
    "Choose the strategy that where the following value is the largest:  \n",
    "\n",
    "$$ p_A + \\sqrt{\\frac{2 log(N)}{n_A}} $$  \n",
    "\n",
    "- $p_A$ is the conversion rate for that version so far  \n",
    "- where $N$ is the total number of rounds (for all sites) \n",
    "- $n_A$ is the number of times site A has been shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Upper Confidence Bound (UCB)\n",
    "-----\n",
    "\n",
    "Proven to have a logarithmic upper bound for regret (hence its name). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You should first make sure to play each bandit once so none of the $n_A$'s are 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Upper Confidence Bound (UCB) Features\n",
    "-----\n",
    "\n",
    "* UCB does __not__ use randomness at all. Unlike epsilon-greedy, it’s possible to know exactly how UCB will behave in any given situation. This can make it easier to reason about at times. \n",
    "\n",
    "* UCB does __not__ have free parameters that you need to configure before you can deploy it. This is a major improvement if you’re interested in running in the wild, because it means that you can start UCB without having clear sense of how you expect the world to behave "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb1(self):\n",
    "    \"\"\"Pick the bandit according to the UCB1 strategy.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    # Make sure to play each bandit at least once\n",
    "    if len(self.trials.nonzero()[0]) < self.n_bandits:\n",
    "        return np.random.randint(0, self.n_bandits)\n",
    "\n",
    "    # Compute the UCB1 values for each bandit\n",
    "    ucb1_vals = np.zeros(len(self.wins))\n",
    "    for i in range(len(self.wins)):\n",
    "        ucb1_vals[i] = (self.wins[i] \n",
    "                        / (self.trials[i]+1.)\n",
    "                        + math.sqrt(2.*math.log(self.N) / self.trials[i]))\n",
    "\n",
    "    # return the max\n",
    "    return np.argmax(ucb1_vals)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further Study\n",
    "------\n",
    "\n",
    "[Boltzmann Exploration Done Right](https://papers.nips.cc/paper/7208-boltzmann-exploration-done-right.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
