{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Multi-Arm Bandits: Reinforcement Learning Simplified</h2></center>\n",
    "\n",
    "<center><img src=\"http://www.offtopia.net/ecai-2012-vago-poster/bandit.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning: A simple definition</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>\"Randomly try strategies.  </center>\n",
    "<center>If they work, choose them more often.\"  </center>\n",
    "<center>‚Äî Raymond Hettinger </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "#### By the end of this session, you should be able to:\n",
    "\n",
    "- Explain multi-arm bandits in your own words.\n",
    "- Explain the exploration / exploitation trade-off.\n",
    "- Define regret in your own words and mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Traditional A/B Testing</h2></center>\n",
    "\n",
    "<center><img src=\"images/ab.png\" height=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Traditional A/B Testing\n",
    "-----\n",
    "\n",
    "First, assign equal numbers of users to Group A and Group B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then, stop the experiment and send all the users to the more successful version of your site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "What is the problem with traditional A/B testing?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Even a small set of decisions can quickly lead to 1,000‚Äôs of unique page layouts. Controlled A/B tests are well suited to simple binary decisions, but they do not scale well to hundreds or thousands of treatments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://blog.acolyer.org/2017/11/22/canopy-an-end-to-end-performance-tracing-and-analysis-system/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Multi-Arm Bandit</h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/b.jpg\" width=\"70%\"/></center>\n",
    "\n",
    "<center>Systematically adjust the numbers of samples for each condition based on current results.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Where does the bandit name come from?\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"http://images.fineartamerica.com/images-medium-large-5/one-arm-bandit-slot-machine-20130308-wingsdomain-art-and-photography.jpg\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Multi-Arm Bandit\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://conversionxl.com/wp-content/uploads/2015/09/multiarmedbandit.jpg\" height=\"500\"/></center>\n",
    "\n",
    "bandits (slot machines) = versions of the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Multi-Arm Bandit Approach \n",
    "-----\n",
    "\n",
    "1) Show a user the version of the site that you think is best most of the time \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "2) As the experiment runs, update the belief about the CTR (Click Through Rate) for each version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "3) Run for however long until satisfied we know the best version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Example Scenario  \n",
    "-----\n",
    "\n",
    "The company you work for is testing out a new version of it's mobile website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "After running an A/B test for an afternoon, the new version of the site appears to performing slightly better than the old version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "After running the test over night, the old version of the site is performing better than the old version with a statistically significant p-value of 0.04.\n",
    "\n",
    "Do you stop the test, or do you keep running it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Pretend you are in a job interview. Pair off. One person answer each question in 30 seconds - 2 minutes. Switch pairs after each question.\n",
    "\n",
    "- List the biggest 3 limitations of traditional A/B testing.\n",
    "- How does MAB address each one?\n",
    "- How does data analysis change between A/B testing and MAB?\n",
    "- Which one is more efficient? Why?\n",
    "- Which one is more ethical? Why?\n",
    "- Why would you choose multi-arm bandits (MAB) over A/B testing?\n",
    "\n",
    "Challenge Question: What are specific situations where you would choose A/B testing over multi-arm bandits (MAB)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Specific situations to choose A/B testing over  multi-arm bandits (MAB)</h2></center>\n",
    " \n",
    "1. Required to have statistical significance (e.g., publishing a paper or your boss is a Statistician).\n",
    " \n",
    "2. Unable to implement multi-arm bandit due to technical debt or system complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/yum.jpg\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If you are going out to eat, should try something new or go with an old favorite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>Exploration vs. Exploitation  </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Language Note</h2></center>\n",
    "\n",
    "__Explore & Exploit__ is conventionally used by the CS / ML, a group not known for their inclusive language.\n",
    "\n",
    "Alternative terms are __Explore & Execute__ (better, but not great)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: [Algorithms to Live By: The Computer Science of Human Decisions ](https://www.amazon.com/Algorithms-Live-Computer-Science-Decisions/dp/1627790365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Exploration vs. Exploitation  </h2></center>\n",
    "\n",
    "<center><img src=\"images/eee.jpg\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exploration\n",
    "------\n",
    "\n",
    "Trying out different options to find the reward associated with the given approach (aka, acquiring more knowledge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exploitation\n",
    "-----\n",
    "\n",
    "Choosing the approach that you believe to have the highest expected payoff (aka, optimizing outcomes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/ee.jpg\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Traditional A/B Testing  </h2></center>\n",
    "\n",
    "A short period of __pure exploration__, in which you assign equal numbers of users to Group A and Group B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A long period of __pure exploitation__, in which you send all of your users to the more successful version of your site and never come back to the option that seemed to be inferior  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "Why might this be a suboptimal strategy?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Potential Inefficiencies  \n",
    "------\n",
    "\n",
    "- Equal number of observations are routed to A and B for a preset amount of time or iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Need to wait for the experiment to conclude for certain statistical guarantees to be provided "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Only after that preset amount of time or iterations do we stop and use the better performer \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- This wastes time and __money__ showing users the suboptimal site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- changes over time - seasonality\n",
    "- fraud\n",
    "- random fluencations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Check for understanding\n",
    "----\n",
    "\n",
    "What are examples of where we can apply bandits to reduce inefficiencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Dynamically A/B testing websites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Adaptive routing in attempts to minimize network delays (either packets üñ• or packages üì¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Clinical trial (I would agrue __NOT__ doing bandits is immoral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Budget allocation amongst competing projects (not a way to make friends)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bezos.jpeg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://medium.com/@savedali/jeff-bezos-guide-to-quitting-medicine-24e16325f159)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is \"Regret\" in Explore / Exploit? </h2></center>\n",
    "\n",
    "The difference of what we actually won vs. what we would expect to win with an optimal strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regret is the cost function we are trying to minimize at a strategic level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Regret Formalism</h2></center>\n",
    "\n",
    "$$ \n",
    "\\begin{align*} \n",
    "\\text{regret} &= \\sum_{i = 1}^k (p_{opt} - p_i)   \\\\ \n",
    "&= k p_{opt} - \\sum_{i = 1}^k p_i\n",
    "\\end{align*} $$\n",
    "\n",
    "where there are k trials and $p_i$ is the probability of winning with the bandit chosen on the i-th run.   \n",
    "\n",
    "$p_{opt}$ is the probability of winning with the best bandit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What would a regret of zero mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A regret of 0 would mean you always made the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "In what situations can you can guarantee zero regret?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is __never__ possible since you need to collect data to determine which variation is the best.\n",
    "\n",
    "<sub>Note that you need to know the true probabilities to calculate the regret. This is a theoretical idea to evaluate which algorithm is best.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "The traditional bandit problem is specified with a discrete and finite number of arms, often indicated by the variable $K$.\n",
    "\n",
    "What optimize technique should you use if you do __not__ have discrete, independent arms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayesian Optimization</h2></center>\n",
    "\n",
    "<center><img src=\"images/bo.jpeg\" width=\"75%\"/></center>\n",
    "\n",
    "A general method for ‚Äúlearning to optimize‚Äù an unknown function.\n",
    "\n",
    "Efficient sampling of all spaces, especially continuous and dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://towardsdatascience.com/shallow-understanding-on-bayesian-optimization-324b6c1f7083)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- In your life, you need to balance exploration (finding what you like) and exploitation (doing what you like).\n",
    "- Multi-arm bandits is a way of optimizing A/B testing by minimizing regret.\n",
    "- Regret is a mathematical term for quantifying not doing the optimal thing.\n",
    "- Multi-arm bandits are a simple version of Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
