{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-Reinforcement-Learning-Applications:-AlphaGo-\" data-toc-modified-id=\"Deep-Reinforcement-Learning-Applications:-AlphaGo--1\">Deep Reinforcement Learning Applications: AlphaGo </a></span></li><li><span><a href=\"#What-does-this-Move-37-mean?\" data-toc-modified-id=\"What-does-this-Move-37-mean?-2\">What does this Move 37 mean?</a></span></li><li><span><a href=\"#Move-37\" data-toc-modified-id=\"Move-37-3\">Move 37</a></span></li><li><span><a href=\"#By-The-End-Of-This-Session-You-Should-Be-Able-To:\" data-toc-modified-id=\"By-The-End-Of-This-Session-You-Should-Be-Able-To:-4\">By The End Of This Session You Should Be Able To:</a></span></li><li><span><a href=\"#Rules-of-Go\" data-toc-modified-id=\"Rules-of-Go-5\">Rules of Go</a></span></li><li><span><a href=\"#Rules-of-Go\" data-toc-modified-id=\"Rules-of-Go-6\">Rules of Go</a></span></li><li><span><a href=\"#Rules-of-Go\" data-toc-modified-id=\"Rules-of-Go-7\">Rules of Go</a></span></li><li><span><a href=\"#Why-is-Go-hard?\" data-toc-modified-id=\"Why-is-Go-hard?-8\">Why is Go hard?</a></span></li><li><span><a href=\"#Why-is-Go-hard?\" data-toc-modified-id=\"Why-is-Go-hard?-9\">Why is Go hard?</a></span></li><li><span><a href=\"#Why-is-Go-hard?-\" data-toc-modified-id=\"Why-is-Go-hard?--10\">Why is Go hard? </a></span></li><li><span><a href=\"#Where-should-we-play-next?\" data-toc-modified-id=\"Where-should-we-play-next?-11\">Where should we play next?</a></span></li><li><span><a href=\"#Policy-Evaluation\" data-toc-modified-id=\"Policy-Evaluation-12\">Policy Evaluation</a></span></li><li><span><a href=\"#-DeepMind's-AlphaGo-Breakthroughs\" data-toc-modified-id=\"-DeepMind's-AlphaGo-Breakthroughs-13\"> DeepMind's AlphaGo Breakthroughs</a></span></li><li><span><a href=\"#Convolutional-Neural-Network-(CNNs)-learn-spatial-relationships\" data-toc-modified-id=\"Convolutional-Neural-Network-(CNNs)-learn-spatial-relationships-14\">Convolutional Neural Network (CNNs) learn spatial relationships</a></span></li><li><span><a href=\"#Convolutional-Neural-Network-(CNNs)-for-Go\" data-toc-modified-id=\"Convolutional-Neural-Network-(CNNs)-for-Go-15\">Convolutional Neural Network (CNNs) for Go</a></span></li><li><span><a href=\"#CNN-for-Value-Function-Approximation-(VFA)\" data-toc-modified-id=\"CNN-for-Value-Function-Approximation-(VFA)-16\">CNN for Value Function Approximation (VFA)</a></span></li><li><span><a href=\"#Supervised-Learning-Based-on-Human-Expert-Play\" data-toc-modified-id=\"Supervised-Learning-Based-on-Human-Expert-Play-17\">Supervised Learning Based-on Human Expert Play</a></span></li><li><span><a href=\"#Monte-Carlo-Tree-Search-(MCTS)-\" data-toc-modified-id=\"Monte-Carlo-Tree-Search-(MCTS)--18\">Monte Carlo Tree Search (MCTS) </a></span></li><li><span><a href=\"#MCTS-Steps\" data-toc-modified-id=\"MCTS-Steps-19\">MCTS Steps</a></span></li><li><span><a href=\"#1.-Selection\" data-toc-modified-id=\"1.-Selection-20\">1. Selection</a></span></li><li><span><a href=\"#2.-Expansion\" data-toc-modified-id=\"2.-Expansion-21\">2. Expansion</a></span></li><li><span><a href=\"#3.-Simulation\" data-toc-modified-id=\"3.-Simulation-22\">3. Simulation</a></span></li><li><span><a href=\"#4.-Backup-/-Backpropagtion\" data-toc-modified-id=\"4.-Backup-/-Backpropagtion-23\">4. Backup / Backpropagtion</a></span></li><li><span><a href=\"#MCTS-Steps\" data-toc-modified-id=\"MCTS-Steps-24\">MCTS Steps</a></span></li><li><span><a href=\"#MCTS-Steps-for-Go\" data-toc-modified-id=\"MCTS-Steps-for-Go-25\">MCTS Steps for Go</a></span></li><li><span><a href=\"#-DeepMind's-AlphaGo-Breakthroughs\" data-toc-modified-id=\"-DeepMind's-AlphaGo-Breakthroughs-26\"> DeepMind's AlphaGo Breakthroughs</a></span></li><li><span><a href=\"#DeepMind's-AlphaGo-Zero-Innovations:-Putting-the-Zero-in-AlphaGo-Zero\" data-toc-modified-id=\"DeepMind's-AlphaGo-Zero-Innovations:-Putting-the-Zero-in-AlphaGo-Zero-27\">DeepMind's AlphaGo Zero Innovations: Putting the Zero in AlphaGo Zero</a></span></li><li><span><a href=\"#AlphaGo-Zero-Policy-Iteration\" data-toc-modified-id=\"AlphaGo-Zero-Policy-Iteration-28\">AlphaGo Zero Policy Iteration</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-29\">Check for understanding</a></span></li><li><span><a href=\"#Value-Iteration-vs-Policy-Iteration\" data-toc-modified-id=\"Value-Iteration-vs-Policy-Iteration-30\">Value Iteration vs Policy Iteration</a></span></li><li><span><a href=\"#Value-Iteration-vs-Policy-Iteration\" data-toc-modified-id=\"Value-Iteration-vs-Policy-Iteration-31\">Value Iteration vs Policy Iteration</a></span></li><li><span><a href=\"#&quot;Training-a-single-AI-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes.&quot;\" data-toc-modified-id=\"&quot;Training-a-single-AI-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes.&quot;-32\">\"Training a single AI model can emit as much carbon as five cars in their lifetimes.\"</a></span></li><li><span><a href=\"#Training-a-single-AI-model-can-[sic]-emit-as-much-carbon-as-five-cars-in-their-lifetimes.\" data-toc-modified-id=\"Training-a-single-AI-model-can-[sic]-emit-as-much-carbon-as-five-cars-in-their-lifetimes.-33\">Training a single AI model can [sic] emit as much carbon as five cars in their lifetimes.</a></span></li><li><span><a href=\"#&quot;can&quot;-is-not-the-same-&quot;do&quot;!\" data-toc-modified-id=\"&quot;can&quot;-is-not-the-same-&quot;do&quot;!-34\">\"can\" is not the same \"do\"!</a></span></li><li><span><a href=\"#Another-take-on-it\" data-toc-modified-id=\"Another-take-on-it-35\">Another take on it</a></span></li><li><span><a href=\"#A-Brief-Cross-Complaining-Rant\" data-toc-modified-id=\"A-Brief-Cross-Complaining-Rant-36\">A Brief Cross-Complaining Rant</a></span></li><li><span><a href=\"#Future-of-Machine-Learning\" data-toc-modified-id=\"Future-of-Machine-Learning-37\">Future of Machine Learning</a></span></li><li><span><a href=\"#Future-of-Programming\" data-toc-modified-id=\"Future-of-Programming-38\">Future of Programming</a></span></li><li><span><a href=\"#Most-difficult-parts-of-Deep-Learning\" data-toc-modified-id=\"Most-difficult-parts-of-Deep-Learning-39\">Most difficult parts of Deep Learning</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-40\">Summary</a></span></li><li><span><a href=\"#Further-Study\" data-toc-modified-id=\"Further-Study-41\">Further Study</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-42\">Bonus Material</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Deep Reinforcement Learning Applications: AlphaGo </h2></center>\n",
    "\n",
    "<center><img src=\"images/go.jpeg\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What does this Move 37 mean?</h2></center>\n",
    "\n",
    "<center><img src=\"https://everpress.imgix.net/img/campaign/original/8405a597caab6eeb-5a597cab1b39a.png?w=700&h=700&bg=ff4d00&fit=fill\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Move 37</h2></center>\n",
    "\n",
    "<center><img src=\"https://i.ytimg.com/vi/JNrXgpSEEIE/maxresdefault.jpg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "During second game of the 5-game match, and was described by commentators, once they got over their initial shock, with words like “beautiful” and “creative.” It left Sedol utterly flumoxed, to the point where he had to spend 15 minutes contemplating his own next move.\n",
    "\n",
    "Move 37 had very high action value, but a very low probability of being played by a human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Describe why the game of Go is challenging for Artificial Intelligence.\n",
    "- List DeepMind's breakthroughs that created a world-champion Go player.\n",
    "- Explain how the Monte Carlo tree search (MCTS) algorithm works.\n",
    "- Explain how AlphaGo Zero is an improvement over the original AlphaGo system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Rules of Go</h2></center>\n",
    "\n",
    "<center><img src=\"https://drericsilverman.files.wordpress.com/2018/08/empty-go-board.png?w=580\" width=\"45%\"/></center>\n",
    "\n",
    "<center>The goal of Go is to control as much of the board as possible. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.britgo.org/intro/intro2.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Rules of Go</h2></center>\n",
    "\n",
    "<center><img src=\"https://i.stack.imgur.com/YkKPk.jpg\" width=\"45%\"/></center>\n",
    "\n",
    "<center>Players take turns placing one of their stones on a unoccupied point.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Rules of Go</h2></center>\n",
    "\n",
    "<center><img src=\"images/capture.gif\" width=\"85%\"/></center>\n",
    "\n",
    "<center>How to capture a piece</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><a href=\"https://www.cosumi.net/en/ \">Let's play Go </a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why is Go hard?</h2></center>\n",
    "\n",
    "<center><img src=\"https://tromp.github.io/img/go/legal19.gif\" width=\"25%\"/></center>\n",
    "\n",
    "Go is very complex!\n",
    "\n",
    "In a 19x19 game board, the number of legal positions is:  $208168199381979984699478633344862770286522453884530548425639456820927419612738015378525648451698519643907259916015628128546089888314427129715319317557736620397247064840935$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "~$2.08\\times10^{170}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source:\n",
    "    \n",
    "- Wikipedia \n",
    "- https://tromp.github.io/go/legal.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why is Go hard?</h2></center>\n",
    "\n",
    "<center><img src=\"images/go_tree.png\" width=\"75%\"/></center>\n",
    "\n",
    "Typical expert-level Go games are ~150 moves with an average of about 250 choices per move.\n",
    "\n",
    "Resulting in a game-tree complexity of $10^{360}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://en.wikipedia.org/wiki/Go_and_mathematics#Complexity_of_certain_Go_configurations) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why is Go hard? </h2></center>\n",
    "\n",
    "<center><img src=\"images/go.jpeg\" width=\"65%\"/></center>\n",
    "\n",
    "<center><h2>Where should we play next?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>How should we decide what is the next move to help win the game?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Policy Evaluation</h2></center>\n",
    "\n",
    "<center><img src=\"https://www.sjeng.org/leelaviz8.png\" width=\"75%\"/></center>\n",
    "\n",
    "<center>How good is our strategy to win in the long-term when picking the next best move?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> DeepMind's AlphaGo Breakthroughs</h2></center>\n",
    "\n",
    "1. Deep Learning with Convolutional Neural Network (CNNs)\n",
    "1. Supervised learning from human experts\n",
    "1. Reinforcement Learning\n",
    "1. Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Convolutional Neural Network (CNNs) learn spatial relationships</h2></center>\n",
    "<center><img src=\"images/cnn.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/features.png\" width=\"60%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Convolutional Neural Network (CNNs) for Go</h2></center>\n",
    "\n",
    "<center><img src=\"images/ encode.png\" width=\"75%\"/></center>\n",
    "\n",
    " <center>AlphaGo sees the board as a binary matrix.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>CNN for Value Function Approximation (VFA)</h2></center>\n",
    "\n",
    "<center><img src=\"images/state.png\" width=\"65%\"/></center> \n",
    "\n",
    "<center>Give a state s, pick the best action a.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Supervised Learning Based-on Human Expert Play</h2></center>\n",
    "\n",
    "AlphaGo use a SL(Supervised Learning)-policy network.\n",
    "\n",
    "The network was 13-layer deep Convolutional Neural Network (CNN) trained on database on ~30 million human expert moves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/inputs.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/policies.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"images/rolloout.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/complete.png\" width=\"75%\"/></center> \n",
    "\n",
    "1. Policy network - What moves are valuable?\n",
    "1. Value network - What board position are valuable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://medium.com/@jonathan_hui/alphago-how-it-works-technically-26ddcc085319) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo Tree Search (MCTS) </h2></center>\n",
    "\n",
    "Use MC simulations to direct a search toward highly-rewarding paths in a decision tree.\n",
    "\n",
    "Find the most promising node then expand the search tree based on random sampling of the search space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>MCTS Steps</h2></center>\n",
    "\n",
    "1. Selection\n",
    "1. Expansion\n",
    "1. Simulation\n",
    "1. Backpropagtion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1. Selection</h2></center>\n",
    "<center><img src=\"images/selection.png\" width=\"40%\"/></center>\n",
    "\n",
    "<center>If each node showing the ratio of wins to total playouts from that point, which  path is the most promising?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Selection: start from root R and select successive child nodes until a leaf node L is reached. The root is the current game state and a leaf is any node from which no simulation (playout) has yet been initiated. The section below says more about a way of biasing choice of child nodes that lets the game tree expand towards the most promising moves, which is the essence of Monte Carlo tree search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>2. Expansion</h2></center>\n",
    "\n",
    "<center><img src=\"images/expansion.png\" width=\"40%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Expansion: unless L ends the game decisively (e.g. win/loss/draw) for either player, create one (or more) child nodes and choose node C from one of them. Child nodes are any valid moves from the game position defined by L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>3. Simulation</h2></center>\n",
    "\n",
    "<center><img src=\"images/simulation.png\" width=\"40%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Simulation: \n",
    "\n",
    "complete one random playout from node C. This step is sometimes also called playout or rollout. \n",
    "\n",
    "A playout may be as simple as choosing uniform random moves until the game is decided (for example in chess, the game is won, lost, or drawn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>4. Backup / Backpropagtion</h2></center>\n",
    "\n",
    "<center><img src=\"images/backprop.png\" width=\"40%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Backpropagation: use the result of the playout to update information in the nodes on the path from C to R. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>MCTS Steps</h2></center>\n",
    "\n",
    "<center><img src=\"images/mtc.png\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Image Source: Sutton and Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>MCTS Steps for Go</h2></center>\n",
    "\n",
    "<center><img src=\"images/mcts_alpha_go.png\" width=\"110%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\n",
    "- [Monte-Carlo Tree Search in Board Games](https://link.springer.com/referenceworkentry/10.1007%2F978-981-4560-50-4_27)\n",
    "- [Monte Carlo Tree Search – beginners guide](https://int8.io/monte-carlo-tree-search-beginners-guide/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> DeepMind's AlphaGo Breakthroughs</h2></center>\n",
    "\n",
    "1. Deep Learning with Convolutional Neural Network (CNNs)\n",
    "1. Supervised learning from human experts\n",
    "1. Reinforcement Learning\n",
    "1. Monte Carlo Tree Search (MCTS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/alphago-zero-banner.jpg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>DeepMind's AlphaGo Zero Innovations: Putting the Zero in AlphaGo Zero</h2></center>\n",
    "\n",
    "- No human data.\n",
    "- No guidance (beyond basic game rules)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Learned exclusively through self-play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>AlphaGo Zero Policy Iteration</h2></center>\n",
    "\n",
    "<center><img src=\"images/self_plan_policy.png\" width=\"55%\"/></center>\n",
    "\n",
    "<center>Implemented a form of policy iteration interleaving policy evaluation with policy improvement.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What is the difference between Value Iteration & Policy Iteration?  \n",
    "Which one is faster per step?  \n",
    "Which one is faster per episode?  \n",
    "Which one will find a better policy more quickly?  \n",
    "\n",
    "Think, Pair, Share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Iteration vs Policy Iteration</h2></center>\n",
    "\n",
    "Policy Iteration:\n",
    "\n",
    "- Start with a random policy, then find the value function of that policy (policy evaluation step).\n",
    "- Then find a new (improved) policy based on the previous value function (policy improvement step).\n",
    "\n",
    "- In this process, each policy is guaranteed to be a strict improvement over the previous one. \n",
    "\n",
    "Value Iteration:\n",
    "\n",
    "- Start with random value function, then find a improved value function in an iterative process.\n",
    "- Stop when optimal value function, no change, or run out of budget.\n",
    "- Can derive easily the optimal policy from the optimal value function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Iteration vs Policy Iteration</h2></center>\n",
    "\n",
    "<center><img src=\"images/3_4.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Policy Iteration - Given a policy, its value function can be obtained using the Bellman operator.\n",
    "- Value Iteration - Values are update with Bellman equations\n",
    "\n",
    "[Source](https://stackoverflow.com/questions/37370015/what-is-the-difference-between-value-iteration-and-policy-iteration) \n",
    "\n",
    "Image Source: 3.4 Sutton and Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"images/compute_diagram-linear@2x-5.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"images/compute_diagram-log@2x-3.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://openai.com/blog/ai-and-compute/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>\"Training a single AI model can emit as much carbon as five cars in their lifetimes.\"</h2></center>\n",
    "\n",
    "<center>from <i>Energy and Policy Considerations for Deep Learning in NLP </i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<center><h2>Training a single AI model can [sic] emit as much carbon as five cars in their lifetimes.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>\"can\" is not the same \"do\"!</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>That paper was napkin math speculation.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Another take on it</h2></center>\n",
    "\n",
    "- Most Deep NLP models are trained by Google.\n",
    "- All Google datacenters are carbon neutral.\n",
    "- Most Google datacenters use renewable power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- All large, professional deep models the training phase is small compare to prediction phase.\n",
    "- Centralized power systems (e.g., powerplants and the \"grid\") are far better for Earth than decentralized power systems (e.g., cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Google claims its AI platform runs on 100 percent renewable energy (citation needed). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>A Brief Cross-Complaining Rant</h2></center>\n",
    "\n",
    "Deep Learning training is benign compared the environment damage done by Bitcoin mining.\n",
    "\n",
    "Bitcoin is almost entirely financial speculation (thus, in no possible way could benefit human-kind) and dwarfs the amount of power used by Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source:\n",
    "\n",
    "- [Energy and Policy Considerations for Deep Learning in NLP](https://drive.google.com/file/d/1v3TxkqPuzvRfiV_RVyRTTFbHl1pZq7Ab/view)\n",
    "- [Google datacenter energy sources](https://storage.googleapis.com/gweb-sustainability.appspot.com/pdf/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Future of Machine Learning</h2></center>\n",
    "\n",
    "The biggest breakthroughs in Machine Learning will __not__ be in supervised Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The biggest breakthroughs will be in:\n",
    "\n",
    "- Unsupervised\n",
    "- Self-supervised\n",
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Future of Programming</h2></center>\n",
    "\n",
    "The biggest breakthroughs in software engineering will __not__ be humans solving more complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The biggest breakthroughs in software engineering will be humans defining problems that computers will find solutions for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.youtube.com/watch?v=giLWCUxhtYM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Most difficult parts of Deep Learning</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Mapping real-world to matrix form, then aligning matrix sizes (input, output, between custom layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Labeled training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Reinforcement Learning needs fewer labels.\n",
    "\n",
    "Games have self-play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Training time / compute resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Summary</h2></center>\n",
    "\n",
    "\n",
    "- Creating an Artificial Intelligence agent to be a world champion Go player is a breakthrough.\n",
    "- DeepMind used a combination of fundamental Deep Reinforcement Learning techniques to do it.\n",
    "- AlphaGo used massive commute to learn Go from scratch.\n",
    "- Deep RL applications require a new set of abilities but have the potential for ground-breaking work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further Study\n",
    "------\n",
    "\n",
    "Please read the original AlphaGo and AlphaGo Zero papers in the readings folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bonus Material</h2></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://avatars3.githubusercontent.com/u/241138?s=460&v=4\" width=\"35%\"/></center>\n",
    "\n",
    "> I like my data large, my algorithms simple, and my labels weak.   \n",
    " > — Andrej Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
