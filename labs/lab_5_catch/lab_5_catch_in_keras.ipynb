{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8c66c4c2b6245b5b73d4f270115d8cc9",
     "grade": false,
     "grade_id": "cell-e4fcb23ff6bbf032",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-Q-learning-to-Play-Catch\" data-toc-modified-id=\"Deep-Q-learning-to-Play-Catch-1\">Deep Q-learning to Play Catch</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#By-the-end-of-this-session,-you-should-be-able-to:\" data-toc-modified-id=\"By-the-end-of-this-session,-you-should-be-able-to:-2.0.1\">By the end of this session, you should be able to:</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e741d27e01d5763feb2efcefe13176d3",
     "grade": false,
     "grade_id": "cell-875f24e3b32f2656",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<center><h2>Deep Q-learning to Play Catch</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "61fedeca47eeeb8ccbd403d243317e83",
     "grade": false,
     "grade_id": "cell-e6e810163b8e1b57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "#### By the end of this session, you should be able to:\n",
    "\n",
    "- Implement the core logic of Experience Replay, including Q-learning.\n",
    "- Explain how Deep Q-learning can learn to play Catch, a simplified version of Pong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f03993ba282dc87aa93fa56c7607e2f4",
     "grade": false,
     "grade_id": "cell-5a4ca566ac3189bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "See other notebooks for general orientation to experience replay, keras, and the game of Catch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3120f621e48a1ea5d70311c6027ed206",
     "grade": false,
     "grade_id": "cell-4d3a2d2bfbbd025c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0fe23323568abffeeea5b08fd63dec5f",
     "grade": false,
     "grade_id": "cell-76933fe6cc8040c3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a628b777a0a4a49e67ed23b19786d87e",
     "grade": false,
     "grade_id": "cell-c99bd89eb220d650",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Import Keras (easy way or hard way)\n",
    "try:\n",
    "    import keras\n",
    "except ImportError:\n",
    "    import pip\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'keras'])\n",
    "    \n",
    "    import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ff95d406c4bbf6ca161009065fa83cf",
     "grade": false,
     "grade_id": "cell-b85f44998a0116cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Catch():\n",
    "    \"\"\"Catch is a simplfied version of Pong.\n",
    "    Catch tries to capture a single pixel â€œfruitâ€ using a three pixel â€œbasketâ€. \n",
    "    The fruit moves down one pixel per step.\n",
    "    Reward of +1 if it catches the fruit and -1 if it misses.\n",
    "    Input: The network sees the entire \"pixels\" grid.\n",
    "    Outputs: 3 actions (move left, stay, move right).\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size=10, basket_size=3, num_actions=3):\n",
    "        self.basket_size = basket_size \n",
    "        self.grid_size = grid_size\n",
    "        self.empty_canvas()\n",
    "        self.reset_state() # Pick random starting location\n",
    "        self.update_canvas()\n",
    "            \n",
    "    def empty_canvas(self):\n",
    "        \"Reset to canvas empty, aka all zeros\"\n",
    "        self.canvas = np.zeros((self.grid_size,)*2)\n",
    "\n",
    "    def act(self, action=1): # Default action is to stay\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        game_over_state = self.is_over()\n",
    "        return self.observe(), reward\n",
    "\n",
    "    def is_over(self):\n",
    "        \"Fruit is at bottom.\"\n",
    "        if self.state[0] >= self.grid_size: # Check fruit row index is at bottom\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_reward(self):\n",
    "        \"Let's see if fruit is in basket or missed.\"\n",
    "        fruit_row, fruit_col, basket = self.state  #[0] # This line is tricky\n",
    "        if self.is_over():\n",
    "            if abs(fruit_col - basket) <= 1:\n",
    "                return 1 # Fruit in basket ðŸ™‚\n",
    "            else:\n",
    "                return -1 # Fruit missed basket â˜¹\n",
    "        else:\n",
    "            return 0 # Carry on ðŸ˜\n",
    "\n",
    "    def observe(self):\n",
    "        \"Convert internal matrix representation into a vector for the input to the MLP DL model.\"\n",
    "        return self.canvas.reshape((1, -1))\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"Pick a new starting place for fruit and basket.\"\n",
    "        n = np.random.randint(low=0, high=self.grid_size)\n",
    "        m = np.random.randint(low=0, high=self.grid_size-2)\n",
    "        self.state = np.asarray([0,  # Row index of fruit \n",
    "                                 n,  # Col index of fruit\n",
    "                                 m]) # Col index of left side of basket (row is always bottom)\n",
    "        \n",
    "    def update_state(self, action_encoded):\n",
    "        \"Given an action, move basket and advance fruit.\"\n",
    "        # Convert encoded action into change in basket index\n",
    "        if action_encoded == 0:   # Left\n",
    "            action_idx = -1\n",
    "        elif action_encoded == 1: # Stay\n",
    "            action_idx = 0\n",
    "        else:\n",
    "            action_idx = 1   # Right\n",
    "\n",
    "        fruit_row_idx, fruit_col_idx, basket_idx = self.state\n",
    "        new_basket_idx = min(max(1, basket_idx+action_idx), self.grid_size-self.basket_size) # Basket moves\n",
    "        fruit_row_idx += 1  # Fruit falls down 1 step\n",
    "        self.state = np.asarray([fruit_row_idx, fruit_col_idx, new_basket_idx])\n",
    "        if not self.is_over():\n",
    "            self.update_canvas()\n",
    "        else:\n",
    "            self.get_reward()\n",
    "            \n",
    "    def update_canvas(self):\n",
    "        \"Read state of fruit and basket, put on canvas.\"\n",
    "        self.empty_canvas()\n",
    "        self.canvas[self.state[0], self.state[1]] = 1  # Draw fruit\n",
    "        self.canvas[-1, self.state[2]:self.state[2]+3] = np.ones(self.basket_size) #.reshape((1, -1))  # Draw basket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3822be22df36a884b71e3c6fec631c63",
     "grade": false,
     "grade_id": "cell-4c468fc6df96498d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Watch a small game to understand the game mechanics\n",
    "c = Catch(grid_size=6)\n",
    "\n",
    "while not c.is_over():\n",
    "    reply = input(\"Press return to make a random move. Press 'q' then return to quit: \") \n",
    "    if reply == \"q\": \n",
    "        break\n",
    "    print(c.canvas) # Show \"screen\"\n",
    "    action = np.random.randint(0, 3) # Randomly select\n",
    "    canvas_snapshot, reward = c.act(action=action) # Make move and see what happens\n",
    "    print(\"reward\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6345e7cec6eaaa63ac5d818f3e54b1ca",
     "grade": false,
     "grade_id": "cell-a703e09501b31a94",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    \"Store the agent's experiences inorder to collect enough example to get a reward signal.\"\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "865c7fd7909159e1f7f55681aad56528",
     "grade": true,
     "grade_id": "cell-9464a1a1d088b127",
     "locked": false,
     "points": 20,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Write the get_batch method for ExperienceReplay class.\n",
    "\n",
    "Each line has been started for you.\n",
    "\n",
    "No tests \n",
    "\n",
    "20 points:\n",
    "-----\n",
    "1 points for num_actions\n",
    "1 points for env_dim\n",
    "9 points for q_sa\n",
    "9 points for targets[i, action_t]\n",
    "\"\"\"\n",
    "\n",
    "class ExperienceReplay(ExperienceReplay): # New class (with same name) inherits everything from old class (with same name)\n",
    "    \n",
    "    def get_batch(self, model, batch_size=10):\n",
    "#         len_memory = len(self.memory)  # Given to you\n",
    "#         num_actions = None # TODO: Read from neural network model\n",
    "#         env_dim =  None # TODO: Read from neural network model\n",
    "#         inputs = np.zeros((min(len_memory, batch_size), env_dim))  # Given to you\n",
    "#         targets = np.zeros((inputs.shape[0], num_actions))  # Given to you\n",
    "#         for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])): # Given to you\n",
    "#             state_t, action_t, reward_t, state_tp1 = self.memory[idx][0] # Given to you\n",
    "#             game_over = self.memory[idx][1] # Given to you\n",
    "#             inputs[i:i+1] = state_t    # Given to you\n",
    "#             q_sa = None # TODO: Find best model prediction for state_tp1\n",
    "#             if game_over:   # Given to you\n",
    "#                 targets[i, action_t] = reward_t # Given to you\n",
    "#             else: # Given to you\n",
    "#                 targets[i, action_t] = None # TODO: Update with Q-learning\n",
    "                \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8cc7dfece694291893876e4fcb4ac061",
     "grade": false,
     "grade_id": "cell-f703763742b2a581",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Keras model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "grid_size = 10\n",
    "num_actions = 3  # [move_left, stay, move_right]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input and first hidden layer\n",
    "model.add(Dense(units=(grid_size*grid_size+num_actions)+15//2,  # Rough rule of thumb is mean of input and output number\n",
    "                input_shape=(grid_size*grid_size,), \n",
    "                activation='relu')) \n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(output_dim=num_actions,\n",
    "          activation='softmax')) \n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss=\"categorical_crossentropy\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35f99de417d07bdfe74ed2ac67720472",
     "grade": false,
     "grade_id": "cell-6506eb61dd6d5363",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run Training\n",
    "\n",
    "# Define environment\n",
    "c = Catch(grid_size=grid_size)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=500)\n",
    "\n",
    "# Exploration rate\n",
    "epsilon = .1  \n",
    "\n",
    "# Training variables\n",
    "n_episodes = 11 # 3_001 is a good number for complete learning\n",
    "win_count = 0\n",
    "history = []\n",
    "loss = float('inf')\n",
    "    \n",
    "for e in range(n_episodes): \n",
    "\n",
    "    if (e == 0) or (e % 10 == 0):\n",
    "        print(f\"Epoch {e:03d}/{n_episodes:,} | Loss {loss:.3f} | Win count {win_count}\")\n",
    "        \n",
    "    # The next new episode.\n",
    "    c.reset_state()\n",
    " \n",
    "    while not c.is_over():\n",
    "        \n",
    "        # Get initial input (as vector).\n",
    "        current_screen = c.observe()\n",
    "        \n",
    "        # Get next action - You guessed it eplison-greedy.\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, num_actions, size=1)\n",
    "        else:\n",
    "            q = model.predict(current_screen)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        # Apply action, get rewards and new state.\n",
    "        future_screen, reward = c.act(action)\n",
    "        if reward == 1:\n",
    "            win_count += 1\n",
    "\n",
    "        # Store experience.\n",
    "        exp_replay.remember([current_screen, action, reward, future_screen], c.is_over())\n",
    "\n",
    "        # Get collected data to train model.\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=50)\n",
    "\n",
    "        # Train model on experiences.\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "        \n",
    "    history.append(win_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7789aa5d66c2484c8311243fb11f13a1",
     "grade": false,
     "grade_id": "cell-064ed3bf4522a5a7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Inspect trained Keras model\n",
    "\n",
    "# Make new game\n",
    "c = Catch(grid_size=10)\n",
    "print(c.canvas)\n",
    "\n",
    "# Given a board sate, what move does the model predict?\n",
    "state = c.observe()\n",
    "model.predict(state)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "44d4633353fc1bd75ba3088ec1da6d98",
     "grade": false,
     "grade_id": "cell-e6987c17f988f77c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "```\n",
    "# Example\n",
    "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]]\n",
    "array([-0.18835554,  0.08628452,  0.3199321 ], dtype=float32)\n",
    "\n",
    "# Predicts move right!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "826799a155bc8678731cfa56a843934f",
     "grade": false,
     "grade_id": "cell-4b7fdd9c46de51a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
