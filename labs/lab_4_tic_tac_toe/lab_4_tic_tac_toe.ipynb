{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "900cff17a287039d8df1ecc6b345dbe0",
     "grade": false,
     "grade_id": "cell-431f49c1a6d7fbf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tic-tac-toe-with-Q-learning\" data-toc-modified-id=\"Tic-tac-toe-with-Q-learning-1\">Tic-tac-toe with Q-learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#By-the-end-of-this-session,-you-should-be-able-to:\" data-toc-modified-id=\"By-the-end-of-this-session,-you-should-be-able-to:-2.0.1\">By the end of this session, you should be able to:</a></span></li></ul></li></ul></li><li><span><a href=\"#Tic-tac-toe-For-The-Win-\" data-toc-modified-id=\"Tic-tac-toe-For-The-Win--3\">Tic-tac-toe For The Win </a></span></li><li><span><a href=\"#-The-Code\" data-toc-modified-id=\"-The-Code-4\"> The Code</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5\">Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "56dd902a8768115c41586497f3222a44",
     "grade": false,
     "grade_id": "cell-1937d89700a8c723",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<center><h2>Tic-tac-toe with Q-learning</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "455f9f755f56388505ca7905d0400bd0",
     "grade": false,
     "grade_id": "cell-a03b93b78b0ab450",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "#### By the end of this session, you should be able to:\n",
    "\n",
    "- Frame tic-tac-toe as a Reinforcement Learning problem.\n",
    "- Create an agent that learns an optimal tic-tac-toe policy through Q-learning.\n",
    "- Write Q-learning from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "12c4b2f57d58e4dd05b86c36331ecf36",
     "grade": false,
     "grade_id": "cell-6bae951432448099",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Tic-tac-toe For The Win \n",
    "-----\n",
    " \n",
    "1. Please start by reading the [Wikipedia article on tic-tac-toe](https://en.wikipedia.org/wiki/Tic-tac-toe) to make sure you understand the game.\n",
    "\n",
    "1. Play a couple of games against a friend to understand the game play.\n",
    "\n",
    "1. Re-read Sutton and Barto's 1.5 \"An Extended Example: Tic-Tac-Toe\"\n",
    "\n",
    "Sample state tree:\n",
    "\n",
    "<center><img src=\"images/tic-tac-toe-tree.png\" width=\"75%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7118a9ddab9dcc8aaf9ee6edaf453594",
     "grade": false,
     "grade_id": "cell-1e1ed5da72cdd6dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For tic-tac-toe, answer the following questions in the following cell:\n",
    "\n",
    "1. How is it sequential?\n",
    "1. What is the environment? \n",
    "1. Who is the agent?\n",
    "1. What are the states?\n",
    "1. What are the rewards?\n",
    "\n",
    "1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1489b2d695f695f866e49e8de6eb5275",
     "grade": true,
     "grade_id": "cell-9975dc99a3d7a9f9",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dee4f4de4d8689f6a1c774f6c5f6f74f",
     "grade": false,
     "grade_id": "cell-a637d694fdcb2488",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " The Code\n",
    " ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ce956f0104014dff463ffaa1a17fa7a5",
     "grade": false,
     "grade_id": "cell-aef5de44dbb8ee54",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9ea5ceb5265ef7512f5220aca6e3b36",
     "grade": false,
     "grade_id": "cell-a7f6a4e84ed4557f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ab6780f1cdbcc4bd0c4bf04553a02ec6",
     "grade": false,
     "grade_id": "cell-df91c7a29e5785ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TicTacToe():\n",
    "    def __init__(self):\n",
    "        self.state = '         ' # Represent state as a vector of strings (not a matrix)\n",
    "        self.player = 'X'\n",
    "        self.winner = None\n",
    "\n",
    "    def allowed_moves(self):\n",
    "        states = []\n",
    "        if self.winner:\n",
    "            return states\n",
    "        for i in range(len(self.state)):\n",
    "            if self.state[i] == ' ':\n",
    "                states.append(self.state[:i] + self.player + self.state[i+1:])\n",
    "        return states\n",
    "\n",
    "    def make_move(self, next_state):\n",
    "        if self.winner:\n",
    "            raise(Exception(\"Game already completed, cannot make another move!\"))\n",
    "        if not self.valid_move(next_state):\n",
    "            raise(Exception(\"Cannot make move {} to {} for player {}\".format(\n",
    "                    self.state, next_state, self.player)))\n",
    "\n",
    "        self.state = next_state\n",
    "        self.winner = self.predict_winner(self.state)\n",
    "        if self.winner:\n",
    "            self.player = None\n",
    "        elif self.player == 'X':\n",
    "            self.player = 'O'\n",
    "        else:\n",
    "            self.player = 'X'\n",
    "\n",
    "    def playable(self):\n",
    "        return ( (not self.winner) and any(self.allowed_moves()) )\n",
    "\n",
    "    def predict_winner(self, state):\n",
    "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "        winner = None\n",
    "        for line in lines:\n",
    "            line_state = state[line[0]] + state[line[1]] + state[line[2]]\n",
    "            if line_state == 'XXX':\n",
    "                winner = 'X'\n",
    "            elif line_state == 'OOO':\n",
    "                winner = 'O'\n",
    "        return winner\n",
    "\n",
    "    def valid_move(self, next_state):\n",
    "        allowed_moves = self.allowed_moves()\n",
    "        if any(state == next_state for state in allowed_moves):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def print_board(self):\n",
    "        s = self.state\n",
    "        print(f'     {s[0]} | {s[1]} | {s[2]} ')\n",
    "        print(f'    -----------')\n",
    "        print(f'     {s[3]} | {s[4]} | {s[5]} ')\n",
    "        print(f'    -----------')\n",
    "        print(f'     {s[6]} | {s[7]} | {s[8]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77f6579a022a9feec06699696711e33f",
     "grade": false,
     "grade_id": "cell-678d24bd872d322a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"All of the Agent expect Q-learning has been given to you.\"\n",
    "    def __init__(self, game_class, epsilon=0.1, alpha=1.0, value_player='X'):\n",
    "        self.V = dict() # Build up the values of different states as we encounter them; Note the Markov assumption\n",
    "        self.NewGame = game_class\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.value_player = value_player\n",
    "\n",
    "    def state_value(self, game_state):\n",
    "        \"Look up state value. If never seen state, then assume neutral.\"\n",
    "        return self.V.get(game_state, 0.0) \n",
    "\n",
    "    def learn_game(self, n_episodes=1_000):\n",
    "        \"Let's learn through complete experience to get that reward.\"\n",
    "        for episode in range(n_episodes):\n",
    "            self.learn_from_episode()\n",
    "\n",
    "    def learn_from_episode(self):\n",
    "        \"Update Values based on reward.\"\n",
    "        game = self.NewGame()\n",
    "        while game.playable():\n",
    "            self.learn_from_move(game)\n",
    "        self.V[game.state] = self.reward(game)\n",
    "        \n",
    "    def learn_select_move(self, game):\n",
    "        \"Exploration and exploitation\"\n",
    "        allowed_state_values = self.state_values( game.allowed_moves() )\n",
    "        if game.player == self.value_player:\n",
    "            best_move = self.argmax_V(allowed_state_values)\n",
    "        else:\n",
    "            best_move = self.argmin_V(allowed_state_values)\n",
    "\n",
    "        selected_move = best_move\n",
    "        if random.random() < self.epsilon:\n",
    "            selected_move = self.random_V(allowed_state_values)\n",
    "\n",
    "        return (best_move, selected_move)\n",
    "\n",
    "    def play_select_move(self, game):\n",
    "        \"Make the move based on the best option for the player.\"\n",
    "        allowed_state_values = self.state_values( game.allowed_moves() )\n",
    "        if game.player == self.value_player:\n",
    "            return self.argmax_V(allowed_state_values)\n",
    "        else:\n",
    "            return self.argmin_V(allowed_state_values)\n",
    "\n",
    "    def demo_game(self, verbose=False):\n",
    "        \"Run a game without learning.\"\n",
    "        game = self.NewGame()\n",
    "        t = 0\n",
    "        while game.playable():\n",
    "            if verbose:\n",
    "                print(\" \\nTurn {}\\n\".format(t))\n",
    "                game.print_board()\n",
    "            move = self.play_select_move(game)\n",
    "            game.make_move(move)\n",
    "            t += 1\n",
    "        if verbose:\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "        if game.winner:\n",
    "            if verbose:\n",
    "                print(\"\\n{} is the winner!\".format(game.winner))\n",
    "            return game.winner\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"\\nIt's a draw!\")\n",
    "            return '-'\n",
    "\n",
    "    def interactive_game(self, agent_player='X'):\n",
    "        \"Play in meatspace.\"\n",
    "        game = self.NewGame()\n",
    "        t = 0\n",
    "        while game.playable():\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "            if game.player == agent_player:\n",
    "                move = self.play_select_move(game)\n",
    "                game.make_move(move)\n",
    "            else:\n",
    "                move = self.request_human_move(game)\n",
    "                game.make_move(move)\n",
    "            t += 1\n",
    "\n",
    "        print(\" \\nTurn {}\\n\".format(t))\n",
    "        game.print_board()\n",
    "\n",
    "        if game.winner:\n",
    "            print(\"\\n{} is the winner!\".format(game.winner))\n",
    "            return game.winner\n",
    "        print(\"\\nIt's a draw!\")\n",
    "        return '-'\n",
    "\n",
    "    def round_V(self):\n",
    "        \"After training, this makes action selection random from equally-good choices\"\n",
    "        for k in self.V.keys():\n",
    "            self.V[k] = round(self.V[k],1)\n",
    "\n",
    "    def state_values(self, game_states):\n",
    "        return dict((state, self.state_value(state)) for state in game_states)\n",
    "\n",
    "    def argmax_V(self, state_values):\n",
    "        \"For the best possible states, chose randomly amongst them.\"\n",
    "        max_V = max(state_values.values())\n",
    "        chosen_state = random.choice([state for state, v in state_values.items() if v == max_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def argmin_V(self, state_values):\n",
    "        \"For the worst possible states, chose randomly amongst them.\"\n",
    "        min_V = min(state_values.values())\n",
    "        chosen_state = random.choice([state for state, v in state_values.items() if v == min_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def random_V(self, state_values):\n",
    "        \"Any state will do.\"\n",
    "        return random.choice(list(state_values.keys()))\n",
    "\n",
    "    def reward(self, game):\n",
    "        if game.winner == self.value_player:\n",
    "            return 1.0 # Winning is good\n",
    "        elif game.winner:\n",
    "            return -1.0 # Losing is bad\n",
    "        else:\n",
    "            return 0.0  # Tying is indifferent\n",
    "\n",
    "    def request_human_move(self, game):\n",
    "        \"Meatspace\"\n",
    "        allowed_moves = [i+1 for i in range(9) if game.state[i] == ' ']\n",
    "        human_move = None\n",
    "        while not human_move:\n",
    "            idx = int(input('Choose move for {}, from {} : '.format(game.player, allowed_moves)))\n",
    "            if any([i==idx for i in allowed_moves]):\n",
    "                human_move = game.state[:idx-1] + game.player + game.state[idx:]\n",
    "        return human_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "checksum": "4c2ed6a6a63b8c07e0a2ac179161124c",
     "grade": true,
     "grade_id": "cell-bc7429735b92e6c4",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Write the learn_from_move method for Agent class.\n",
    "\n",
    "Each line has been started for you.\n",
    "See lecture notes for pseudocode.\n",
    "\n",
    "No tests (I couldn't figure a fair way of testing).\n",
    "\n",
    "15 points:\n",
    "-----\n",
    "\n",
    "5 points for correctly setting up Q-learning\n",
    "10 points for Q-learning line\n",
    "\"\"\"\n",
    "\n",
    "class Agent(Agent): # Note: New class (with same name) inherits from old class (with same name)\n",
    "    def learn_from_move(self, game):\n",
    "        \"The heart of Q-learning.\"\n",
    "        \n",
    "        current_state = None\n",
    "        best_next_move, selected_next_move = None, None\n",
    "        r = None\n",
    "        \n",
    "        current_state_value = None\n",
    "        best_move_value = None\n",
    "        td_target = None\n",
    "        self.V[current_state] = None # This is Q-learning. The previous lines setup this line. \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        game.make_move(selected_next_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c4db075c4ef0dbc422957b1f48544ec",
     "grade": false,
     "grade_id": "cell-facd8329031e20e9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def print_demo_game_stats(agent, n_games=100):\n",
    "    \"Run demo to get sample statistics, no learning\"\n",
    "    long_name = {'X': 'X wins', \n",
    "               'O': 'O Wins',\n",
    "               '-': 'Tie'}\n",
    "    results = Counter([agent.demo_game() for _ in range(n_games)])\n",
    "    game_stats = {long_name[k]: v/n_games for k, v in results.items()}\n",
    "    for k in long_name.values():\n",
    "        print(f\"{k:<7}: {game_stats.get(k, 0):^8.2%}\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "def train(agent, training_block_size = 1_000, n_training_blocks = 10):\n",
    "    \"Given agent, do more training. Return (hopefully) improved agent.\"\n",
    "    print(\"Before learning:\")\n",
    "    print_demo_game_stats(agent, n_games=1_000)\n",
    "    for n_training_block in range(1, n_training_blocks+1):\n",
    "        agent.learn_game(training_block_size)\n",
    "        \n",
    "        print(f\"After {training_block_size*n_training_block:,} learning games:\")\n",
    "        print_demo_game_stats(agent)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2149a4d83e5719d6577c6273316b4a71",
     "grade": false,
     "grade_id": "cell-52a8b3678487a28d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's if the agent learns ðŸ¤ž\n",
    "agent = Agent(TicTacToe, \n",
    "              epsilon = 0.1,\n",
    "              alpha = 1 \n",
    "             )\n",
    "agent = train(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "427e4a56521aac1d21a7166769d993eb",
     "grade": false,
     "grade_id": "cell-54ad7bb6f74621af",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your training should look something like this:\n",
    "\n",
    "```\n",
    "Before learning:\n",
    "X wins :  56.00% \n",
    "O Wins :  31.50% \n",
    "Tie    :  12.50% \n",
    "\n",
    "After 1,000 learning games:\n",
    "X wins :  56.00% \n",
    "O Wins :  38.00% \n",
    "Tie    :  6.00%  \n",
    "\n",
    "After 2,000 learning games:\n",
    "X wins :  53.00% \n",
    "O Wins :  29.00% \n",
    "Tie    :  18.00% \n",
    "\n",
    "After 3,000 learning games:\n",
    "X wins :  46.00% \n",
    "O Wins :  34.00% \n",
    "Tie    :  20.00% \n",
    "\n",
    "After 4,000 learning games:\n",
    "X wins :  35.00% \n",
    "O Wins :  18.00% \n",
    "Tie    :  47.00% \n",
    "\n",
    "After 5,000 learning games:\n",
    "X wins :  20.00% \n",
    "O Wins :  5.00%  \n",
    "Tie    :  75.00% \n",
    "\n",
    "After 6,000 learning games:\n",
    "X wins :  16.00% \n",
    "O Wins :  3.00%  \n",
    "Tie    :  81.00% \n",
    "\n",
    "After 7,000 learning games:\n",
    "X wins :  9.00%  \n",
    "O Wins :  2.00%  \n",
    "Tie    :  89.00% \n",
    "\n",
    "After 8,000 learning games:\n",
    "X wins :  8.00%  \n",
    "O Wins :  0.00%  \n",
    "Tie    :  92.00% \n",
    "\n",
    "After 9,000 learning games:\n",
    "X wins :  2.00%  \n",
    "O Wins :  1.00%  \n",
    "Tie    :  97.00% \n",
    "\n",
    "After 10,000 learning games:\n",
    "X wins :  0.00%  \n",
    "O Wins :  2.00%  \n",
    "Tie    :  98.00% \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "41bfe8c46a342be926a10285dd1a7d79",
     "grade": false,
     "grade_id": "cell-d76525bc9c523c1e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Let's peak at first couple of states and their values\")\n",
    "print('State   ', '\\t', 'Value')\n",
    "for k, v in list(agent.V.items())[0:10]:\n",
    "    print(k, '\\t', v),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b668b4481b66b1a43ecfbcc98db921d",
     "grade": false,
     "grade_id": "cell-dde04911446d5a70",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Your State/Value pairs should look something like this:\n",
    "\n",
    "```\n",
    "State    \t Value\n",
    "None \t None\n",
    "          \t 0.0\n",
    "     X    \t 0.0\n",
    "  O  X    \t 0.0\n",
    "  O XX    \t 0.0\n",
    "O O XX    \t 1.0\n",
    "OXO XX    \t 1.0\n",
    "OXOOXX    \t 1.0\n",
    "OXOOXX  X \t -1.0\n",
    "OXOOXX OX \t 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "329e8ed5175439fbea36b1d2c7821595",
     "grade": false,
     "grade_id": "cell-86aac898fe3992eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Demo a single game\n",
    "agent.demo_game(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a3f36f609209b08bc0b150916f227286",
     "grade": false,
     "grade_id": "cell-0c88226ba2afe402",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this lab, what does epsilon do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "725a5f0a9c09d3e4bc6e9252ff104a3c",
     "grade": true,
     "grade_id": "cell-04dcaf227bb0b438",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6e9a1721e744d172b32058e7b736c4ef",
     "grade": false,
     "grade_id": "cell-d16e16a0a1be8447",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this lab, why is `alpha = 1`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "eceabe55f8e07968bc71517452b84d87",
     "grade": true,
     "grade_id": "cell-282561436e8fddcb",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b8da65ca16898fa18d139104594f97b0",
     "grade": false,
     "grade_id": "cell-d4c216f0a010ddc1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Even though tic-tac-toe is a simple game, you don't need to be able to explicitly define the optimal strategy to write an agent that can discover it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
