{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84cc50e0103dd426e931abace80a5a88",
     "grade": false,
     "grade_id": "cell-cd506a83f2749987",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#CartPole,-aka-Inverted-Pendulum\" data-toc-modified-id=\"CartPole,-aka-Inverted-Pendulum-1\">CartPole, aka Inverted Pendulum</a></span></li><li><span><a href=\"#OpenAI's-CartPole-Environment\" data-toc-modified-id=\"OpenAI's-CartPole-Environment-2\">OpenAI's CartPole Environment</a></span></li><li><span><a href=\"#Define-Environment\" data-toc-modified-id=\"Define-Environment-3\">Define Environment</a></span></li><li><span><a href=\"#Define-Neural-Network-\" data-toc-modified-id=\"Define-Neural-Network--4\">Define Neural Network </a></span></li><li><span><a href=\"#Define-Agent\" data-toc-modified-id=\"Define-Agent-5\">Define Agent</a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-6\">Train the model</a></span></li><li><span><a href=\"#Test-the-model\" data-toc-modified-id=\"Test-the-model-7\">Test the model</a></span></li><li><span><a href=\"#Grading-Submission-Notes\" data-toc-modified-id=\"Grading-Submission-Notes-8\">Grading Submission Notes</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-9\">Bonus Material</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2a4bccc66a232ad3baf80b0e22c83feb",
     "grade": false,
     "grade_id": "cell-d1bf5f4c89e7e2d5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<center><h2>CartPole, aka Inverted Pendulum</h2></center>\n",
    "<br>\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Cart-pendulum.svg/300px-Cart-pendulum.svg.png\" width=\"35%\"/></center>\n",
    "<br><br><br>\n",
    "<center><a href=\"https://fluxml.ai/experiments/cartPole/\">Demo!</a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de304ab8b94f598a628c965a11361cfc",
     "grade": false,
     "grade_id": "cell-c0d33663507134a1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<center><h2>OpenAI's CartPole Environment</h2></center>\n",
    "\n",
    "A pole is attached to a cart by an un-actuated joint, and the cart moves along a frictionless track. \n",
    "\n",
    "The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "\n",
    "The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "\n",
    "A reward of +1 is provided for every time-step that the pole remains upright. \n",
    "\n",
    "The episode ends when:\n",
    "\n",
    "- The pole is more than 15 degrees from the vertical.\n",
    "- The cart moves more than 2.4 units from the center.\n",
    "- 200 time-steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "acbe7ba8495197b24d84336768f73cb5",
     "grade": false,
     "grade_id": "cell-a51a351b15ae43c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7ab348952dd1c0302563083976902724",
     "grade": false,
     "grade_id": "cell-c82cd832ad799334",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "938110b661752e74cf98191e5a174181",
     "grade": false,
     "grade_id": "cell-921bed7526659b48",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Define Environment\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "90e5cda29f2c95574536ed7686d35783",
     "grade": false,
     "grade_id": "cell-a0ee8aaeb24a3546",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Import OpenAI's gym (easy way or hard way)\n",
    "try:\n",
    "    import gym \n",
    "except ImportError:\n",
    "    import pip\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'gym'])\n",
    "    \n",
    "    import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93904432c1b95e6d5a57a7cd0e7c43f7",
     "grade": false,
     "grade_id": "cell-5d203326bbc89312",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "85bbd86889fffc7b6f4ec8eeba2fe922",
     "grade": false,
     "grade_id": "cell-3586114c91bc0040",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size:   4\n",
      "Action size:  2\n"
     ]
    }
   ],
   "source": [
    "# Let's see how the RL problem is formulated\n",
    "print('State size:  ', env.observation_space.shape[0] )\n",
    "print('Action size: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "603faac5d821fa44645cf4d8bba56169",
     "grade": false,
     "grade_id": "cell-ca36a212eeea495d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's read the docs:\n",
    "https://github.com/openai/gym/wiki/CartPole-v0\n",
    "\n",
    "Let's read the code: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd3efb95c63d33b5557410adb0b4a300",
     "grade": false,
     "grade_id": "cell-8bcc9787c79a0cad",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Define Neural Network \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ab6b8acc906271eba31ba0d03e8222e5",
     "grade": false,
     "grade_id": "cell-3e745c5cae78ac45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f60659f5dae34813b36125d1c71ac79f",
     "grade": false,
     "grade_id": "cell-e85f9aa5db75db9f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a sample model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(env.action_space.n, activation='linear'))\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6cbb776e4cc6dfbe80a1d6dd6a1ca5bb",
     "grade": true,
     "grade_id": "cell-2231bf938a827285",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 170\n",
      "Trainable params: 170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define your model\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d6e9d5979de5e3d9bcf3c213964e7af",
     "grade": false,
     "grade_id": "cell-bbdd905ca8215804",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Define Agent\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b0571bb97d3f1dab6055c96e19b5e2d9",
     "grade": false,
     "grade_id": "cell-2f0f6c70252694f2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Import keras-rl's gym (easy way or hard way)\n",
    "try:\n",
    "    import rl\n",
    "except ImportError:\n",
    "    import pip\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'keras-rl'])\n",
    "    import rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b1204d86cab0e5c29e1de1f48a1dce63",
     "grade": false,
     "grade_id": "cell-49f915c829089899",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Define memory\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "memory = SequentialMemory(limit=50_000, \n",
    "                          window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f375db04db6d68ae0bd971f057fb7ab9",
     "grade": false,
     "grade_id": "cell-b0e77786def2fac8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Define sample policy\n",
    "\n",
    "from rl.policy import GreedyQPolicy\n",
    "\n",
    "policy = GreedyQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "037fd4a1f51fe736715ef10986690d64",
     "grade": true,
     "grade_id": "cell-e3d15f2a794ccbad",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Define your policy\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "55c30f41999a1d46422ff72526708bd6",
     "grade": false,
     "grade_id": "cell-159c7468132cfafb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a sample agent\n",
    "from rl.agents.dqn import DQNAgent\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=env.action_space.n, \n",
    "               memory=memory,\n",
    "               nb_steps_warmup=15,\n",
    "               target_model_update=1e-2,\n",
    "               policy=policy)\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "51a02a28cefb67538e358f8799e16d14",
     "grade": true,
     "grade_id": "cell-dabdbe3e55483280",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Define your agent\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3b96be6fa693d0120b75a0a1701f71f1",
     "grade": false,
     "grade_id": "cell-4af37f6d4f991bdc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Train the model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2e62edb915274068fa4e59cfd9980fed",
     "grade": false,
     "grade_id": "cell-9909f10b95abf8f9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1500 steps ...\n",
      "   10/1500: episode: 1, duration: 0.501s, episode steps: 10, steps per second: 20, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-3.091, 1.935], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   20/1500: episode: 2, duration: 2.538s, episode steps: 10, steps per second: 4, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.029, 1.951], loss: 0.526494, mean_absolute_error: 0.530830, mean_q: 0.384968\n",
      "   33/1500: episode: 3, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.139 [-2.931, 1.722], loss: 0.418333, mean_absolute_error: 0.468051, mean_q: 0.473482\n",
      "   42/1500: episode: 4, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.810, 1.770], loss: 0.329586, mean_absolute_error: 0.411586, mean_q: 0.615536\n",
      "   50/1500: episode: 5, duration: 0.045s, episode steps: 8, steps per second: 177, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.574, 1.545], loss: 0.290200, mean_absolute_error: 0.414596, mean_q: 0.734394\n",
      "   60/1500: episode: 6, duration: 0.069s, episode steps: 10, steps per second: 145, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.049, 1.949], loss: 0.256723, mean_absolute_error: 0.425439, mean_q: 0.856683\n",
      "   70/1500: episode: 7, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.031, 1.998], loss: 0.255595, mean_absolute_error: 0.450296, mean_q: 0.895844\n",
      "   80/1500: episode: 8, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.033, 1.995], loss: 0.254073, mean_absolute_error: 0.481575, mean_q: 1.024470\n",
      "   88/1500: episode: 9, duration: 0.152s, episode steps: 8, steps per second: 53, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-2.588, 1.553], loss: 0.249406, mean_absolute_error: 0.509519, mean_q: 1.132087\n",
      "   98/1500: episode: 10, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.031, 1.950], loss: 0.271340, mean_absolute_error: 0.554962, mean_q: 1.193469\n",
      "  107/1500: episode: 11, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.871, 1.752], loss: 0.274057, mean_absolute_error: 0.592413, mean_q: 1.276731\n",
      "  116/1500: episode: 12, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.176 [-2.869, 1.734], loss: 0.271298, mean_absolute_error: 0.621069, mean_q: 1.302531\n",
      "  125/1500: episode: 13, duration: 0.044s, episode steps: 9, steps per second: 206, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.848, 1.774], loss: 0.302121, mean_absolute_error: 0.678634, mean_q: 1.387465\n",
      "  133/1500: episode: 14, duration: 0.052s, episode steps: 8, steps per second: 154, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-2.504, 1.614], loss: 0.306885, mean_absolute_error: 0.719901, mean_q: 1.443916\n",
      "  143/1500: episode: 15, duration: 0.093s, episode steps: 10, steps per second: 108, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-3.001, 1.968], loss: 0.299995, mean_absolute_error: 0.758748, mean_q: 1.485914\n",
      "  151/1500: episode: 16, duration: 0.056s, episode steps: 8, steps per second: 143, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.534, 1.608], loss: 0.298800, mean_absolute_error: 0.788479, mean_q: 1.520879\n",
      "  160/1500: episode: 17, duration: 0.056s, episode steps: 9, steps per second: 162, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.778, 1.719], loss: 0.264892, mean_absolute_error: 0.812461, mean_q: 1.634634\n",
      "  170/1500: episode: 18, duration: 0.050s, episode steps: 10, steps per second: 202, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-3.104, 1.922], loss: 0.307757, mean_absolute_error: 0.873049, mean_q: 1.710122\n",
      "  178/1500: episode: 19, duration: 0.041s, episode steps: 8, steps per second: 197, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.174 [-2.584, 1.538], loss: 0.286263, mean_absolute_error: 0.935417, mean_q: 1.799374\n",
      "  187/1500: episode: 20, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.168 [-2.894, 1.716], loss: 0.307721, mean_absolute_error: 0.996495, mean_q: 1.817686\n",
      "  196/1500: episode: 21, duration: 0.044s, episode steps: 9, steps per second: 204, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.772, 1.745], loss: 0.379781, mean_absolute_error: 1.074899, mean_q: 1.844073\n",
      "  205/1500: episode: 22, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.831, 1.761], loss: 0.305983, mean_absolute_error: 1.084815, mean_q: 1.884819\n",
      "  213/1500: episode: 23, duration: 0.045s, episode steps: 8, steps per second: 176, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.539, 1.539], loss: 0.324030, mean_absolute_error: 1.118685, mean_q: 1.953043\n",
      "  222/1500: episode: 24, duration: 0.064s, episode steps: 9, steps per second: 141, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.811, 1.791], loss: 0.326004, mean_absolute_error: 1.132044, mean_q: 2.003495\n",
      "  231/1500: episode: 25, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.181 [-2.905, 1.745], loss: 0.283445, mean_absolute_error: 1.152735, mean_q: 2.061958\n",
      "  241/1500: episode: 26, duration: 0.049s, episode steps: 10, steps per second: 204, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.107, 1.917], loss: 0.338602, mean_absolute_error: 1.238294, mean_q: 2.198782\n",
      "  251/1500: episode: 27, duration: 0.049s, episode steps: 10, steps per second: 202, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.967, 1.920], loss: 0.295328, mean_absolute_error: 1.210952, mean_q: 2.154241\n",
      "  260/1500: episode: 28, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.840, 1.744], loss: 0.329653, mean_absolute_error: 1.302962, mean_q: 2.328090\n",
      "  269/1500: episode: 29, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.854, 1.810], loss: 0.270942, mean_absolute_error: 1.283212, mean_q: 2.361068\n",
      "  278/1500: episode: 30, duration: 0.064s, episode steps: 9, steps per second: 140, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.849, 1.775], loss: 0.299476, mean_absolute_error: 1.306185, mean_q: 2.444097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  289/1500: episode: 31, duration: 0.105s, episode steps: 11, steps per second: 105, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-3.291, 2.106], loss: 0.328887, mean_absolute_error: 1.357420, mean_q: 2.473842\n",
      "  299/1500: episode: 32, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.123 [-2.732, 1.779], loss: 0.383178, mean_absolute_error: 1.416042, mean_q: 2.524676\n",
      "  309/1500: episode: 33, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.132 [-2.684, 1.730], loss: 0.300767, mean_absolute_error: 1.398077, mean_q: 2.582131\n",
      "  319/1500: episode: 34, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-3.044, 1.937], loss: 0.316852, mean_absolute_error: 1.378293, mean_q: 2.652113\n",
      "  329/1500: episode: 35, duration: 0.054s, episode steps: 10, steps per second: 187, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.127 [-2.990, 1.974], loss: 0.307067, mean_absolute_error: 1.352071, mean_q: 2.729672\n",
      "  339/1500: episode: 36, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-3.102, 1.925], loss: 0.250710, mean_absolute_error: 1.204858, mean_q: 2.796037\n",
      "  349/1500: episode: 37, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-3.025, 1.951], loss: 0.286323, mean_absolute_error: 1.194598, mean_q: 2.870263\n",
      "  358/1500: episode: 38, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.763, 1.750], loss: 0.316359, mean_absolute_error: 1.199046, mean_q: 2.941188\n",
      "  368/1500: episode: 39, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-2.980, 1.994], loss: 0.313798, mean_absolute_error: 1.154791, mean_q: 2.980377\n",
      "  377/1500: episode: 40, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.174 [-2.893, 1.773], loss: 0.323264, mean_absolute_error: 1.128999, mean_q: 3.027468\n",
      "  387/1500: episode: 41, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.116 [-3.015, 1.977], loss: 0.294387, mean_absolute_error: 1.115880, mean_q: 3.027670\n",
      "  397/1500: episode: 42, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.111 [-2.970, 1.970], loss: 0.313893, mean_absolute_error: 1.105224, mean_q: 3.112324\n",
      "  406/1500: episode: 43, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.855, 1.756], loss: 0.260412, mean_absolute_error: 1.076913, mean_q: 3.197607\n",
      "  416/1500: episode: 44, duration: 0.050s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.115 [-3.031, 1.983], loss: 0.282075, mean_absolute_error: 1.084555, mean_q: 3.191386\n",
      "  425/1500: episode: 45, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.813, 1.749], loss: 0.274913, mean_absolute_error: 1.078101, mean_q: 3.216347\n",
      "  433/1500: episode: 46, duration: 0.042s, episode steps: 8, steps per second: 189, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-2.524, 1.599], loss: 0.254768, mean_absolute_error: 1.086093, mean_q: 3.240295\n",
      "  441/1500: episode: 47, duration: 0.044s, episode steps: 8, steps per second: 180, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.176 [-2.609, 1.561], loss: 0.278551, mean_absolute_error: 1.099541, mean_q: 3.264241\n",
      "  451/1500: episode: 48, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.093, 1.939], loss: 0.200561, mean_absolute_error: 1.049333, mean_q: 3.344331\n",
      "  461/1500: episode: 49, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.076, 1.967], loss: 0.232496, mean_absolute_error: 1.061212, mean_q: 3.415807\n",
      "  471/1500: episode: 50, duration: 0.075s, episode steps: 10, steps per second: 134, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.994, 1.921], loss: 0.237729, mean_absolute_error: 1.080078, mean_q: 3.490668\n",
      "  480/1500: episode: 51, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.168 [-2.835, 1.749], loss: 0.214445, mean_absolute_error: 1.101056, mean_q: 3.441227\n",
      "  489/1500: episode: 52, duration: 0.077s, episode steps: 9, steps per second: 117, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.842, 1.744], loss: 0.230637, mean_absolute_error: 1.138223, mean_q: 3.496484\n",
      "  498/1500: episode: 53, duration: 0.090s, episode steps: 9, steps per second: 101, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.804, 1.768], loss: 0.171339, mean_absolute_error: 1.102019, mean_q: 3.675139\n",
      "  507/1500: episode: 54, duration: 0.127s, episode steps: 9, steps per second: 71, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.837, 1.777], loss: 0.233264, mean_absolute_error: 1.157490, mean_q: 3.616342\n",
      "  515/1500: episode: 55, duration: 0.076s, episode steps: 8, steps per second: 105, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.525, 1.537], loss: 0.210998, mean_absolute_error: 1.179927, mean_q: 3.526154\n",
      "  525/1500: episode: 56, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.170 [-3.146, 1.945], loss: 0.233997, mean_absolute_error: 1.197672, mean_q: 3.616598\n",
      "  534/1500: episode: 57, duration: 0.084s, episode steps: 9, steps per second: 108, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.178 [-2.912, 1.779], loss: 0.192755, mean_absolute_error: 1.198915, mean_q: 3.691528\n",
      "  544/1500: episode: 58, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.035, 1.957], loss: 0.145828, mean_absolute_error: 1.186245, mean_q: 3.817939\n",
      "  554/1500: episode: 59, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-3.031, 1.984], loss: 0.195863, mean_absolute_error: 1.226267, mean_q: 3.741994\n",
      "  562/1500: episode: 60, duration: 0.058s, episode steps: 8, steps per second: 137, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.568, 1.584], loss: 0.167412, mean_absolute_error: 1.229452, mean_q: 3.748212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  572/1500: episode: 61, duration: 0.073s, episode steps: 10, steps per second: 138, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.072, 1.965], loss: 0.157984, mean_absolute_error: 1.229580, mean_q: 4.004125\n",
      "  581/1500: episode: 62, duration: 0.064s, episode steps: 9, steps per second: 140, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.844, 1.752], loss: 0.174511, mean_absolute_error: 1.249547, mean_q: 3.956228\n",
      "  589/1500: episode: 63, duration: 0.076s, episode steps: 8, steps per second: 105, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.510, 1.598], loss: 0.171492, mean_absolute_error: 1.259190, mean_q: 3.887810\n",
      "  599/1500: episode: 64, duration: 0.082s, episode steps: 10, steps per second: 122, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-3.017, 1.959], loss: 0.176594, mean_absolute_error: 1.271010, mean_q: 3.897130\n",
      "  607/1500: episode: 65, duration: 0.191s, episode steps: 8, steps per second: 42, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.564, 1.608], loss: 0.113407, mean_absolute_error: 1.248027, mean_q: 4.080200\n",
      "  616/1500: episode: 66, duration: 0.092s, episode steps: 9, steps per second: 98, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.842, 1.752], loss: 0.133961, mean_absolute_error: 1.262355, mean_q: 4.150860\n",
      "  626/1500: episode: 67, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.089, 1.982], loss: 0.156336, mean_absolute_error: 1.270354, mean_q: 4.018519\n",
      "  635/1500: episode: 68, duration: 0.116s, episode steps: 9, steps per second: 77, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.862, 1.777], loss: 0.155344, mean_absolute_error: 1.272328, mean_q: 4.010862\n",
      "  644/1500: episode: 69, duration: 0.112s, episode steps: 9, steps per second: 81, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.179 [-2.906, 1.761], loss: 0.105582, mean_absolute_error: 1.263435, mean_q: 4.260314\n",
      "  654/1500: episode: 70, duration: 0.138s, episode steps: 10, steps per second: 73, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-3.009, 1.999], loss: 0.110665, mean_absolute_error: 1.237540, mean_q: 4.212770\n",
      "  663/1500: episode: 71, duration: 0.130s, episode steps: 9, steps per second: 69, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.175 [-2.845, 1.731], loss: 0.115267, mean_absolute_error: 1.258940, mean_q: 4.325068\n",
      "  672/1500: episode: 72, duration: 0.120s, episode steps: 9, steps per second: 75, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.816, 1.810], loss: 0.128102, mean_absolute_error: 1.246096, mean_q: 4.175339\n",
      "  682/1500: episode: 73, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.071, 1.995], loss: 0.116081, mean_absolute_error: 1.263434, mean_q: 4.383150\n",
      "  690/1500: episode: 74, duration: 0.069s, episode steps: 8, steps per second: 116, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.172 [-2.586, 1.557], loss: 0.102851, mean_absolute_error: 1.252471, mean_q: 4.311900\n",
      "  700/1500: episode: 75, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.965, 1.938], loss: 0.104220, mean_absolute_error: 1.250411, mean_q: 4.372543\n",
      "  710/1500: episode: 76, duration: 0.190s, episode steps: 10, steps per second: 53, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-3.076, 1.937], loss: 0.091170, mean_absolute_error: 1.246632, mean_q: 4.386611\n",
      "  719/1500: episode: 77, duration: 0.099s, episode steps: 9, steps per second: 91, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.757, 1.758], loss: 0.091825, mean_absolute_error: 1.273645, mean_q: 4.465165\n",
      "  728/1500: episode: 78, duration: 0.087s, episode steps: 9, steps per second: 104, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.172 [-2.855, 1.750], loss: 0.102558, mean_absolute_error: 1.262658, mean_q: 4.390295\n",
      "  739/1500: episode: 79, duration: 0.113s, episode steps: 11, steps per second: 97, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.137 [-2.978, 1.923], loss: 0.112380, mean_absolute_error: 1.304864, mean_q: 4.426174\n",
      "  748/1500: episode: 80, duration: 0.125s, episode steps: 9, steps per second: 72, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-2.841, 1.749], loss: 0.068538, mean_absolute_error: 1.304942, mean_q: 4.532626\n",
      "  757/1500: episode: 81, duration: 0.104s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-2.752, 1.781], loss: 0.052312, mean_absolute_error: 1.329306, mean_q: 4.805836\n",
      "  766/1500: episode: 82, duration: 0.091s, episode steps: 9, steps per second: 99, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.782, 1.776], loss: 0.076192, mean_absolute_error: 1.311306, mean_q: 4.484338\n",
      "  775/1500: episode: 83, duration: 0.110s, episode steps: 9, steps per second: 81, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.756, 1.724], loss: 0.108205, mean_absolute_error: 1.351923, mean_q: 4.578633\n",
      "  784/1500: episode: 84, duration: 0.102s, episode steps: 9, steps per second: 89, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.736, 1.714], loss: 0.079592, mean_absolute_error: 1.332451, mean_q: 4.588233\n",
      "  793/1500: episode: 85, duration: 0.090s, episode steps: 9, steps per second: 100, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.812, 1.728], loss: 0.049519, mean_absolute_error: 1.352577, mean_q: 4.783137\n",
      "  803/1500: episode: 86, duration: 0.094s, episode steps: 10, steps per second: 106, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.123 [-2.956, 1.926], loss: 0.057717, mean_absolute_error: 1.362543, mean_q: 4.742175\n",
      "  812/1500: episode: 87, duration: 0.095s, episode steps: 9, steps per second: 95, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.834, 1.716], loss: 0.050445, mean_absolute_error: 1.341610, mean_q: 4.632189\n",
      "  822/1500: episode: 88, duration: 0.106s, episode steps: 10, steps per second: 95, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-3.070, 1.932], loss: 0.053880, mean_absolute_error: 1.359520, mean_q: 4.816351\n",
      "  832/1500: episode: 89, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.011, 1.959], loss: 0.036154, mean_absolute_error: 1.330209, mean_q: 4.724120\n",
      "  842/1500: episode: 90, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-3.064, 1.927], loss: 0.050182, mean_absolute_error: 1.378591, mean_q: 4.943112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  851/1500: episode: 91, duration: 0.098s, episode steps: 9, steps per second: 92, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.838, 1.786], loss: 0.068909, mean_absolute_error: 1.338026, mean_q: 4.586572\n",
      "  861/1500: episode: 92, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.771, 1.767], loss: 0.036889, mean_absolute_error: 1.362355, mean_q: 4.867910\n",
      "  870/1500: episode: 93, duration: 0.086s, episode steps: 9, steps per second: 105, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.795, 1.793], loss: 0.041245, mean_absolute_error: 1.351378, mean_q: 4.826426\n",
      "  879/1500: episode: 94, duration: 0.096s, episode steps: 9, steps per second: 94, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.816, 1.742], loss: 0.040797, mean_absolute_error: 1.363711, mean_q: 4.849209\n",
      "  889/1500: episode: 95, duration: 0.088s, episode steps: 10, steps per second: 114, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-3.048, 1.903], loss: 0.031073, mean_absolute_error: 1.368264, mean_q: 4.917035\n",
      "  897/1500: episode: 96, duration: 0.057s, episode steps: 8, steps per second: 141, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.175 [-2.596, 1.539], loss: 0.051894, mean_absolute_error: 1.407957, mean_q: 5.060041\n",
      "  905/1500: episode: 97, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.523, 1.533], loss: 0.024361, mean_absolute_error: 1.404626, mean_q: 5.040098\n",
      "  915/1500: episode: 98, duration: 0.065s, episode steps: 10, steps per second: 153, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.682, 1.728], loss: 0.035507, mean_absolute_error: 1.393440, mean_q: 4.798463\n",
      "  925/1500: episode: 99, duration: 0.088s, episode steps: 10, steps per second: 114, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.158 [-2.789, 1.748], loss: 0.034357, mean_absolute_error: 1.439594, mean_q: 4.953092\n",
      "  936/1500: episode: 100, duration: 0.095s, episode steps: 11, steps per second: 115, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.133 [-2.619, 1.722], loss: 0.035570, mean_absolute_error: 1.439124, mean_q: 4.937596\n",
      "  945/1500: episode: 101, duration: 0.087s, episode steps: 9, steps per second: 104, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.172 [-2.543, 1.548], loss: 0.089905, mean_absolute_error: 1.475772, mean_q: 4.956312\n",
      "  954/1500: episode: 102, duration: 0.079s, episode steps: 9, steps per second: 114, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.136 [-2.464, 1.537], loss: 0.019632, mean_absolute_error: 1.501428, mean_q: 5.163634\n",
      "  964/1500: episode: 103, duration: 0.082s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.668, 1.791], loss: 0.049060, mean_absolute_error: 1.467497, mean_q: 4.893626\n",
      "  973/1500: episode: 104, duration: 0.086s, episode steps: 9, steps per second: 105, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.155 [-2.465, 1.549], loss: 0.034695, mean_absolute_error: 1.520279, mean_q: 5.073645\n",
      "  983/1500: episode: 105, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.163 [-2.751, 1.717], loss: 0.028278, mean_absolute_error: 1.501845, mean_q: 4.919753\n",
      "  992/1500: episode: 106, duration: 0.087s, episode steps: 9, steps per second: 103, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.150 [-2.498, 1.531], loss: 0.020211, mean_absolute_error: 1.559841, mean_q: 5.280114\n",
      " 1002/1500: episode: 107, duration: 0.107s, episode steps: 10, steps per second: 94, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.146 [-2.703, 1.746], loss: 0.029412, mean_absolute_error: 1.533451, mean_q: 5.155451\n",
      " 1011/1500: episode: 108, duration: 0.082s, episode steps: 9, steps per second: 110, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.139 [-2.451, 1.525], loss: 0.055329, mean_absolute_error: 1.543163, mean_q: 5.163546\n",
      " 1020/1500: episode: 109, duration: 0.083s, episode steps: 9, steps per second: 109, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.785, 1.798], loss: 0.029560, mean_absolute_error: 1.509862, mean_q: 5.009205\n",
      " 1029/1500: episode: 110, duration: 0.089s, episode steps: 9, steps per second: 101, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.169 [-2.520, 1.530], loss: 0.028401, mean_absolute_error: 1.534869, mean_q: 5.238990\n",
      " 1039/1500: episode: 111, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.159 [-2.788, 1.777], loss: 0.018904, mean_absolute_error: 1.460034, mean_q: 4.941160\n",
      " 1049/1500: episode: 112, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.140 [-2.697, 1.714], loss: 0.022396, mean_absolute_error: 1.500117, mean_q: 5.226220\n",
      " 1059/1500: episode: 113, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.144 [-2.759, 1.768], loss: 0.050766, mean_absolute_error: 1.516411, mean_q: 5.379154\n",
      " 1068/1500: episode: 114, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.142 [-2.525, 1.604], loss: 0.057430, mean_absolute_error: 1.501085, mean_q: 5.234901\n",
      " 1077/1500: episode: 115, duration: 0.052s, episode steps: 9, steps per second: 173, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.129 [-2.459, 1.571], loss: 0.011805, mean_absolute_error: 1.576030, mean_q: 5.573816\n",
      " 1086/1500: episode: 116, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.150 [-2.499, 1.549], loss: 0.056496, mean_absolute_error: 1.534316, mean_q: 5.163584\n",
      " 1096/1500: episode: 117, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.174 [-2.778, 1.710], loss: 0.053799, mean_absolute_error: 1.608888, mean_q: 5.405744\n",
      " 1106/1500: episode: 118, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.144 [-2.749, 1.768], loss: 0.026858, mean_absolute_error: 1.614647, mean_q: 5.401349\n",
      " 1115/1500: episode: 119, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.139 [-2.467, 1.553], loss: 0.023233, mean_absolute_error: 1.540855, mean_q: 5.139405\n",
      " 1124/1500: episode: 120, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.498, 1.575], loss: 0.052481, mean_absolute_error: 1.555437, mean_q: 5.194659\n",
      " 1134/1500: episode: 121, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.149 [-2.763, 1.745], loss: 0.021684, mean_absolute_error: 1.546543, mean_q: 5.117630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1142/1500: episode: 122, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.503, 1.558], loss: 0.019777, mean_absolute_error: 1.588554, mean_q: 5.243237\n",
      " 1152/1500: episode: 123, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-2.435, 1.597], loss: 0.085578, mean_absolute_error: 1.651400, mean_q: 5.336720\n",
      " 1162/1500: episode: 124, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.132 [-2.696, 1.714], loss: 0.051693, mean_absolute_error: 1.611200, mean_q: 5.196894\n",
      " 1171/1500: episode: 125, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.160 [-2.478, 1.538], loss: 0.048237, mean_absolute_error: 1.614564, mean_q: 5.178157\n",
      " 1180/1500: episode: 126, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.519, 1.595], loss: 0.016349, mean_absolute_error: 1.653178, mean_q: 5.343961\n",
      " 1189/1500: episode: 127, duration: 0.058s, episode steps: 9, steps per second: 157, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.475, 1.558], loss: 0.064228, mean_absolute_error: 1.724370, mean_q: 5.490877\n",
      " 1198/1500: episode: 128, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.161 [-2.522, 1.517], loss: 0.022492, mean_absolute_error: 1.595650, mean_q: 5.059822\n",
      " 1207/1500: episode: 129, duration: 0.048s, episode steps: 9, steps per second: 189, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.813, 1.718], loss: 0.019328, mean_absolute_error: 1.603213, mean_q: 5.118098\n",
      " 1216/1500: episode: 130, duration: 0.063s, episode steps: 9, steps per second: 144, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.105 [-2.419, 1.606], loss: 0.019298, mean_absolute_error: 1.668045, mean_q: 5.399150\n",
      " 1227/1500: episode: 131, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.098 [-2.594, 1.796], loss: 0.113986, mean_absolute_error: 1.679024, mean_q: 5.211174\n",
      " 1237/1500: episode: 132, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.105 [-2.354, 1.596], loss: 0.019634, mean_absolute_error: 1.773331, mean_q: 5.532650\n",
      " 1247/1500: episode: 133, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.146 [-2.722, 1.733], loss: 0.023405, mean_absolute_error: 1.762385, mean_q: 5.469490\n",
      " 1255/1500: episode: 134, duration: 0.048s, episode steps: 8, steps per second: 168, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.571, 1.582], loss: 0.021253, mean_absolute_error: 1.724616, mean_q: 5.332642\n",
      " 1264/1500: episode: 135, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.124 [-2.478, 1.606], loss: 0.017179, mean_absolute_error: 1.801217, mean_q: 5.692786\n",
      " 1274/1500: episode: 136, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.155 [-2.756, 1.737], loss: 0.094410, mean_absolute_error: 1.728648, mean_q: 5.247871\n",
      " 1284/1500: episode: 137, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.138 [-2.674, 1.758], loss: 0.050833, mean_absolute_error: 1.845997, mean_q: 5.704078\n",
      " 1293/1500: episode: 138, duration: 0.065s, episode steps: 9, steps per second: 138, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.134 [-2.512, 1.605], loss: 0.014639, mean_absolute_error: 1.738993, mean_q: 5.207794\n",
      " 1303/1500: episode: 139, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.110 [-2.393, 1.601], loss: 0.052251, mean_absolute_error: 1.792576, mean_q: 5.248352\n",
      " 1313/1500: episode: 140, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.108 [-2.383, 1.605], loss: 0.047191, mean_absolute_error: 1.845946, mean_q: 5.468320\n",
      " 1322/1500: episode: 141, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.112 [-2.423, 1.600], loss: 0.016759, mean_absolute_error: 1.717908, mean_q: 5.006408\n",
      " 1331/1500: episode: 142, duration: 0.070s, episode steps: 9, steps per second: 128, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.155 [-2.511, 1.542], loss: 0.052416, mean_absolute_error: 1.779541, mean_q: 5.134660\n",
      " 1340/1500: episode: 143, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.142 [-2.167, 1.322], loss: 0.016290, mean_absolute_error: 1.809713, mean_q: 5.179570\n",
      " 1350/1500: episode: 144, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.121 [-2.681, 1.789], loss: 0.051621, mean_absolute_error: 1.892272, mean_q: 5.375869\n",
      " 1361/1500: episode: 145, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.132 [-2.607, 1.710], loss: 0.021191, mean_absolute_error: 1.932254, mean_q: 5.558347\n",
      " 1370/1500: episode: 146, duration: 0.045s, episode steps: 9, steps per second: 198, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.773, 1.764], loss: 0.054987, mean_absolute_error: 1.873597, mean_q: 5.407499\n",
      " 1380/1500: episode: 147, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.114 [-2.680, 1.774], loss: 0.026633, mean_absolute_error: 1.779310, mean_q: 5.036762\n",
      " 1389/1500: episode: 148, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.476, 1.564], loss: 0.019981, mean_absolute_error: 1.781420, mean_q: 5.042478\n",
      " 1398/1500: episode: 149, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-2.799, 1.735], loss: 0.016983, mean_absolute_error: 1.835527, mean_q: 5.248470\n",
      " 1407/1500: episode: 150, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.150 [-2.489, 1.549], loss: 0.020335, mean_absolute_error: 1.773094, mean_q: 4.971084\n",
      " 1417/1500: episode: 151, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.732, 1.711], loss: 0.017098, mean_absolute_error: 1.788199, mean_q: 5.042079\n",
      " 1427/1500: episode: 152, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.123 [-2.741, 1.808], loss: 0.016532, mean_absolute_error: 1.808548, mean_q: 5.171590\n",
      " 1436/1500: episode: 153, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.844, 1.776], loss: 0.018835, mean_absolute_error: 1.845195, mean_q: 5.324759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1445/1500: episode: 154, duration: 0.067s, episode steps: 9, steps per second: 134, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.150 [-2.497, 1.527], loss: 0.056756, mean_absolute_error: 1.776425, mean_q: 4.981745\n",
      " 1455/1500: episode: 155, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.984, 1.969], loss: 0.016073, mean_absolute_error: 1.841001, mean_q: 5.221635\n",
      " 1463/1500: episode: 156, duration: 0.046s, episode steps: 8, steps per second: 176, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.594, 1.538], loss: 0.022991, mean_absolute_error: 1.827514, mean_q: 5.114954\n",
      " 1473/1500: episode: 157, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.671, 1.734], loss: 0.021320, mean_absolute_error: 1.867131, mean_q: 5.241817\n",
      " 1482/1500: episode: 158, duration: 0.045s, episode steps: 9, steps per second: 201, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.139 [-2.446, 1.536], loss: 0.024200, mean_absolute_error: 1.900629, mean_q: 5.259531\n",
      " 1491/1500: episode: 159, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.138 [-2.472, 1.599], loss: 0.015384, mean_absolute_error: 1.832270, mean_q: 5.056315\n",
      " 1500/1500: episode: 160, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.169 [-2.819, 1.716], loss: 0.021803, mean_absolute_error: 1.882801, mean_q: 5.279412\n",
      "done, took 14.778 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb3aa55240>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "dqn.fit(env, \n",
    "        nb_steps=1_500, \n",
    "        visualize=False, \n",
    "        verbose=2 # Episode logging\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f3b9d2bda92baa39266103e29c43e6f4",
     "grade": false,
     "grade_id": "cell-75523f1f1614a9ba",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Test the model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aecb6432709a14141e29ee5b3a948a3b",
     "grade": false,
     "grade_id": "cell-dc76d501f98398d4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 11 episodes ...\n",
      "Episode 1: reward: 9.000, steps: 9\n",
      "Episode 2: reward: 10.000, steps: 10\n",
      "Episode 3: reward: 10.000, steps: 10\n",
      "Episode 4: reward: 9.000, steps: 9\n",
      "Episode 5: reward: 10.000, steps: 10\n",
      "Episode 6: reward: 10.000, steps: 10\n",
      "Episode 7: reward: 8.000, steps: 8\n",
      "Episode 8: reward: 9.000, steps: 9\n",
      "Episode 9: reward: 9.000, steps: 9\n",
      "Episode 10: reward: 9.000, steps: 9\n",
      "Episode 11: reward: 10.000, steps: 10\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test_results = dqn.test(env, nb_episodes=11, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed026d841bb9578fff36d7b4631a68e0",
     "grade": false,
     "grade_id": "cell-3403b66e6626a82e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There current model gets 9.50 steps.\n"
     ]
    }
   ],
   "source": [
    "# The max is 200 steps per eposide.\n",
    "# The goal of the assignment to train an agent that performs about 180 (on average).\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "# Remove worst run\n",
    "test_results.history['episode_reward'].remove(min(test_results.history['episode_reward']))\n",
    "\n",
    "# Take the average the remaining runs\n",
    "test_performance = mean(test_results.history['episode_reward'])\n",
    "\n",
    "print(f\"There current model gets {test_performance:.2f} steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1885c78d5ace99d0ada20204e600c8d9",
     "grade": true,
     "grade_id": "cell-ba1d196810b21f40",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-99-e353aa7f68bd>\", line 3, in <module>\n",
      "    assert test_performance > 100.00\n",
      "AssertionError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AssertionError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/brian/anaconda3/envs/rl-course/lib/python3.7/posixpath.py\", line 383, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 5 points for over 100 steps per eposide.\n",
    "\n",
    "assert test_performance > 100.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "49c46e87d216cd8a2f3992659dd86ee5",
     "grade": true,
     "grade_id": "cell-ca8771487a5c5a1f",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 5 points for over 125 steps per eposide.\n",
    "\n",
    "assert test_performance > 125.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c598e496a7210572b150b925288f7271",
     "grade": true,
     "grade_id": "cell-71810e3bdbc554a0",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 points for over 170 steps per eposide.\n",
    "\n",
    "assert test_performance > 170.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a293e3e6639b6de918f324ccefa3a22c",
     "grade": false,
     "grade_id": "cell-8ae4c53ab1f8b7b5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Grading Submission Notes\n",
    "-------\n",
    "\n",
    "If there is output, we'll grade your submitted lab without running the notebook. If there is __not__ output, we'll run the notebook to get output to grade.\n",
    "\n",
    "It would behove you to submit a notebook with output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6ea1476c8282571e4a139b320ed725d9",
     "grade": false,
     "grade_id": "cell-bfffd0af5bee7e15",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dc2cd3d066c652938b988dca3a3c6965",
     "grade": false,
     "grade_id": "cell-7dd98b9ed96a3e94",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Bonus Material\n",
    "-----\n",
    "\n",
    "Learn more about CartPole from the physics and control-model perspective  [here](https://danielpiedrahita.wordpress.com/portfolio/cart-pole-control/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "52720afe915a870f3d188c7b95ee3519",
     "grade": false,
     "grade_id": "cell-daf528ab83a826b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
