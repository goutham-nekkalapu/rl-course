{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dynamic-Programming,-Temporal-difference-(TD)-learning,-and-Friends\" data-toc-modified-id=\"Dynamic-Programming,-Temporal-difference-(TD)-learning,-and-Friends-1\">Dynamic Programming, Temporal difference (TD) learning, and Friends</a></span></li><li><span><a href=\"#By-The-End-Of-This-Session-You-Should-Be-Able-To:\" data-toc-modified-id=\"By-The-End-Of-This-Session-You-Should-Be-Able-To:-2\">By The End Of This Session You Should Be Able To:</a></span></li><li><span><a href=\"#Breaking-down-Dynamic-Programming-(DP)\" data-toc-modified-id=\"Breaking-down-Dynamic-Programming-(DP)-3\">Breaking down Dynamic Programming (DP)</a></span></li><li><span><a href=\"#How-does-Dynamic-Programming-(DP)-work?\" data-toc-modified-id=\"How-does-Dynamic-Programming-(DP)-work?-4\">How does Dynamic Programming (DP) work?</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-5\">Check for understanding</a></span></li><li><span><a href=\"#-Student-Activity\" data-toc-modified-id=\"-Student-Activity-6\"> Student Activity</a></span></li><li><span><a href=\"#Value\" data-toc-modified-id=\"Value-7\">Value</a></span></li><li><span><a href=\"#Value-Function-\" data-toc-modified-id=\"Value-Function--8\">Value Function </a></span></li><li><span><a href=\"#Value-Function\" data-toc-modified-id=\"Value-Function-9\">Value Function</a></span></li><li><span><a href=\"#Markov-decision-process-(MDP)-&amp;-Dynamic-Programming-(DP)\" data-toc-modified-id=\"Markov-decision-process-(MDP)-&amp;-Dynamic-Programming-(DP)-10\">Markov decision process (MDP) &amp; Dynamic Programming (DP)</a></span></li><li><span><a href=\"#Value-Function\" data-toc-modified-id=\"Value-Function-11\">Value Function</a></span></li><li><span><a href=\"#Utility-vs.-Value-Function\" data-toc-modified-id=\"Utility-vs.-Value-Function-12\">Utility vs. Value Function</a></span></li><li><span><a href=\"#2-methods-of-finding-the-Value-Function-\" data-toc-modified-id=\"2-methods-of-finding-the-Value-Function--13\">2 methods of finding the Value Function </a></span></li><li><span><a href=\"#Value-Iteration-\" data-toc-modified-id=\"Value-Iteration--14\">Value Iteration </a></span></li><li><span><a href=\"#Back-to-Gridworld\" data-toc-modified-id=\"Back-to-Gridworld-15\">Back to Gridworld</a></span></li><li><span><a href=\"#-Policy-Iteration-Intuition\" data-toc-modified-id=\"-Policy-Iteration-Intuition-16\"> Policy Iteration Intuition</a></span></li><li><span><a href=\"#-Policy-Iteration-Intuition\" data-toc-modified-id=\"-Policy-Iteration-Intuition-17\"> Policy Iteration Intuition</a></span></li><li><span><a href=\"#-Policy-Iteration\" data-toc-modified-id=\"-Policy-Iteration-18\"> Policy Iteration</a></span></li><li><span><a href=\"#Policy-Iteration-Thought-Experiment\" data-toc-modified-id=\"Policy-Iteration-Thought-Experiment-19\">Policy Iteration Thought Experiment</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-20\">Check for understanding</a></span></li><li><span><a href=\"#-Policy-Iteration-Formalism\" data-toc-modified-id=\"-Policy-Iteration-Formalism-21\"> Policy Iteration Formalism</a></span></li><li><span><a href=\"#Value-Iteration-vs-Policy-Iteration\" data-toc-modified-id=\"Value-Iteration-vs-Policy-Iteration-22\">Value Iteration vs Policy Iteration</a></span></li><li><span><a href=\"#Monte-Carlo-(MC)-Methods\" data-toc-modified-id=\"Monte-Carlo-(MC)-Methods-23\">Monte Carlo (MC) Methods</a></span></li><li><span><a href=\"#Monte-Carlo-(MC)-Intuition\" data-toc-modified-id=\"Monte-Carlo-(MC)-Intuition-24\">Monte Carlo (MC) Intuition</a></span></li><li><span><a href=\"#Monte-Carlo-(MC)-Demo\" data-toc-modified-id=\"Monte-Carlo-(MC)-Demo-25\">Monte Carlo (MC) Demo</a></span></li><li><span><a href=\"#Monte-Carlo-(MC)-Demo-Results\" data-toc-modified-id=\"Monte-Carlo-(MC)-Demo-Results-26\">Monte Carlo (MC) Demo Results</a></span></li><li><span><a href=\"#Compare-DP-vs.-MC\" data-toc-modified-id=\"Compare-DP-vs.-MC-27\">Compare DP vs. MC</a></span></li><li><span><a href=\"#Temporal-difference-(TD)-learning\" data-toc-modified-id=\"Temporal-difference-(TD)-learning-28\">Temporal difference (TD) learning</a></span></li><li><span><a href=\"#TD(0)-is-the-simpilest\" data-toc-modified-id=\"TD(0)-is-the-simpilest-29\">TD(0) is the simpilest</a></span></li><li><span><a href=\"#-Value-Estimation-is-at-the-heart-of-TD\" data-toc-modified-id=\"-Value-Estimation-is-at-the-heart-of-TD-30\"> Value Estimation is at the heart of TD</a></span></li><li><span><a href=\"#Compare-the-3-Algorithms\" data-toc-modified-id=\"Compare-the-3-Algorithms-31\">Compare the 3 Algorithms</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-32\">Check for understanding</a></span></li><li><span><a href=\"#Compare-algorithms-for-space-complexity\" data-toc-modified-id=\"Compare-algorithms-for-space-complexity-33\">Compare algorithms for space complexity</a></span></li><li><span><a href=\"#-Summary\" data-toc-modified-id=\"-Summary-34\"> Summary</a></span></li><li><span><a href=\"#-Summary\" data-toc-modified-id=\"-Summary-35\"> Summary</a></span></li><li><span><a href=\"#-Bonus-Material\" data-toc-modified-id=\"-Bonus-Material-36\"> Bonus Material</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Dynamic Programming, Temporal difference (TD) learning, and Friends</h2></center>\n",
    "\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/travelling_salesman_problem.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Write a dynamic programming function in Python.\n",
    "- Explain in your own words the following concepts:\n",
    "    - How dynamic programming (DP) is useful in Reinforcement Learning\n",
    "    - Value Function\n",
    "    - Policy Iteration\n",
    "    - Monte Carlo (MC) \n",
    "    - Temporal difference (TD) learning\n",
    "- Compare and contrast the following:\n",
    "    - Value Iteration vs. Policy Iteration\n",
    "    - DP vs. MC vs. TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Breaking down Dynamic Programming (DP)</h2></center>\n",
    " \n",
    "Dynamic (vs static) - A sequential component to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Programming - Mathematical programming (aka, mathematical optimization). See also {Integer, Linear, Convex, ‚Ä¶} Programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dynamic Programming is an optimization technique for sequential problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How does Dynamic Programming (DP) work?</h2></center>\n",
    "\n",
    "A method for solving complex problems by breaking them down into simpler subproblems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Problem elements:\n",
    "\n",
    "- Optimal substructure - Solving subproblems optimally can optimally solve original problem\n",
    "- Overlapping subproblem - Dividing the problem makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reduces computations by storing and reusing partial results.\n",
    "\n",
    "One of the best examples of divide-and-conquer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "<center>What are examples of problems that can be solved with dynamic programming (DP)?</center>\n",
    "\n",
    "General types ‚Ä¶\n",
    "Specific examples ‚Ä¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Shortest path - From here to the another point (e.g., graph path)\n",
    "1. Generating sequences that depend previous elements (e.g., Fibonacci)\n",
    "1. Maximizing a sequence (e.g., cumulative sum with the constraints)\n",
    "1. Scheduling problems (e.g., weighted interval scheduling)\n",
    "1. String algorithms (e.g., sequence alignment for DNA)\n",
    "1. Markov decision process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Student Activity</h2></center>\n",
    "\n",
    "Solve the following problems with DP:\n",
    "\n",
    "1. Generate a Fibonacci Sequence: `0, 1, 1, 2, 3, 5, 8, 13`\n",
    "1. Find maximum cumulative sum with the constraint of not taking two numbers in a row.\n",
    "```python\n",
    "assert max_constrained([1, 2, 3, 1])     ==  4\n",
    "assert max_constrained([2, 1, 1, 2])     ==  4\n",
    "assert max_constrained([2, 7, 9, 3, 1])  == 12\n",
    "```\n",
    "1. Find the Longest Increasing Subsequence (LIS) \n",
    "```python\n",
    "assert lis([10, 22, 9]) == 2\n",
    "assert lis([10, 22, 9, 10, 21, 50]) == 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Other problems with DP solutions\n",
    "\n",
    "- Given two strings, find the longest common substring.\n",
    "- Find the longest increasing subsequence.\n",
    "- Determine the minimum (or unique) number of ways to make n cents, given coins of denominations less than n cents.\n",
    "- Given a knapsack with a total weight capacity and a list of items with weight w(i) and value v(i), determine the max total value you can carry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import lru_cache \n",
    "\n",
    "@lru_cache()\n",
    "def fib_dp(n_th):\n",
    "    \"Calculate nth Fibonacci number using dynamic programming\"\n",
    "    if n_th == 0: return 0\n",
    "    if n_th == 1: return 1\n",
    "    return fib_dp(n_th-1) + fib_dp(n_th-2)\n",
    "\n",
    "[fib_dp(n_th) for n_th in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Brian‚Äôs other solutions to generate Fibonacci Sequence](https://github.com/brianspiering/fibonacci_sequences)\n",
    "\n",
    "There are many ways of applying DP to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def max_constrained(nums):\n",
    "    \"Get maximum cumulative sum with the constraint of not taking two numbers in a row.\"\n",
    "    total_current, total_previous = 0, 0\n",
    "\n",
    "    for n in nums: \n",
    "        total_previous, total_current = total_current, max(total_previous + n, total_current)\n",
    "\n",
    "    return total_current\n",
    "\n",
    "assert max_constrained([1, 2, 3, 1])     ==  4\n",
    "assert max_constrained([2, 1, 1, 2])     ==  4\n",
    "assert max_constrained([2, 7, 9, 3, 1])  == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def longest_increasing_subsequence(nums): \n",
    "    n = len(nums)\n",
    "    seq_len = [1]*n # Initialize LIS values for all indexes \n",
    "    \n",
    "    # Compute the length of each increasing subsequence\n",
    "    for i in range(1, n): \n",
    "        for j in range(0, i): \n",
    "            if (nums[i] > nums[j]) and (seq_len[i] < (seq_len[j] + 1)): \n",
    "                seq_len[i] = seq_len[j]+1\n",
    "\n",
    "    return max(seq_len) # Find longest subsequence\n",
    "\n",
    "assert longest_increasing_subsequence([10, 22, 9]) == 2\n",
    "assert longest_increasing_subsequence([10, 22, 9, 10, 21, 50]) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Original Source](https://www.geeksforgeeks.org/python-program-for-longest-increasing-subsequence/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " A Fibonacci sequence is also a type of cumulative sum.\n",
    "\n",
    "The House Robber just adds a deterministic selection function to pick the value moving forward.\n",
    "\n",
    "In fact, some types of Reinforcement Learning are extensions to the House Robber by adding stochastic selection or even a choice (e.g., bandit) to maximize cumulative sum (i.e., reward). Just swap out max with your own favorite custom function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value</h2></center>\n",
    "\n",
    "The sum of all future rewards that can be achieved from a state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Œ≥ will have big effect on how rewards are mapped to value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Function </h2></center> \n",
    "\n",
    "- $ùëâ_ùúã$ is the estimated the expected returns by following a given policy $œÄ$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Remember a policy is mapping from state to action) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Value Function attempts to find the optimal $œÄ^*$ that maximizes the estimated expected returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Function</h2></center>\n",
    "\n",
    "$$ùëâ_ùúã(s) = E[U |s,   œÄ ]$$\n",
    "\n",
    "The Value Function is expected the utility associated with following policy œÄ from the state s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Markov decision process (MDP) & Dynamic Programming (DP)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bellman equation is recursive decomposition.\n",
    "\n",
    "Bellman equation states the optimal solution is a combination of the next step and future steps.\n",
    "    \n",
    "$$U(s) = R(s) + Œ≥ ‚Ä¢ max \\sum_{s‚Ä≤} P(s‚Ä≤ | s,a)U(s‚Ä≤)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Value function stores and reuses solutions, thus the dynamic programming component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Function</h2></center>\n",
    "<center><img src=\"images/value_function.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Utility vs. Value Function</h2></center> \n",
    "\n",
    "Utility - $U(s)$ is the expected value of being in a state and the discounted utility based on actions available in that state.\n",
    "\n",
    "$$U(s) = R(s) + Œ≥ max \\sum_{s‚Ä≤}P(s‚Ä≤ | s,a)U(s‚Ä≤)$$\n",
    "\n",
    "Value Function - $ùëâ_ùúã(s)$ is sum of all actions following a policy\n",
    "\n",
    "<center><img src=\"images/value_function_simple.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Value Function is the chaining of Utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>2 methods of finding the Value Function </h2></center>\n",
    "\n",
    "1. Value iteration\n",
    "2. Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Iteration </h2></center>\n",
    "\n",
    "Calculate the estimated utility of each state.\n",
    "\n",
    "Then use those calculated state utilities to select an optimal action in each state.\n",
    "\n",
    "The collection of optimal action-state pairs is the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Back to Gridworld</h2></center>\n",
    "\n",
    "<table><tr><td><img src=\"images/wharehouse_with_neg_rewards.png\" /></td>\n",
    "    <td><img src=\"images/utility.png\"/></td></tr></table>\n",
    "<center>Rewards   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Utilities</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Assuming full knowledge of the MDP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Policy Iteration Intuition</h2></center>\n",
    "\n",
    "Initialize a starting policy. \n",
    "\n",
    "What would be a reasonable good choice for starting policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Random walk.\n",
    "\n",
    "(Brian's favorite baseline is random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Policy Iteration Intuition</h2></center>\n",
    "\n",
    "Initialize a starting policy. `\n",
    "\n",
    "Repeat until satisfied:\n",
    "\n",
    "1. __Policy Evaluation__ - For given policy, compute the estimated utility (that estimated utility is value function $v_œÄ$).\n",
    "1. __Policy Improvement__ - Pick next, best (greedy) policy available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Policy Iteration</h2></center>\n",
    "<center><img src=\"images/iteration.png\" width=\"30%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Policy Iteration Thought Experiment</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_with_neg_rewards.png\" width=\"30%\"/></center>\n",
    "\n",
    "Round #1 Given $œÄ_a$: R, R, R, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Round #2 Given $œÄ_b$: R, R, U, U, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Round #3 Given $œÄ_c$: U, U, R, R, R\n",
    "\n",
    "(Note: We are exploring and exploiting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>$œÄ_c$ is the winner, aka $œÄ_*$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What are examples of being \"satisfied\"? When would we stop looking for a better policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Guaranteed optimal\n",
    "1. Good enough\n",
    "1. Run out of resource budget (could be time or number of steps)\n",
    "1. Stops improving "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The book and video course use \"Stops improving\" because they are theoreticians. In the applied world, \"good enough\" and \"out of budget\" are typically when projects stop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Policy Iteration Formalism</h2></center>\n",
    "\n",
    "1. Policy Evaluation:\n",
    "\n",
    "$$v_  œÄ(s) = ùîº[ùëà_{t+1}+Œ≥ùëà_{t+2}+‚Ä¶|S_t=s]$$\n",
    "\n",
    "1. Policy Improvement:\n",
    "\n",
    "$$œÄ' = greedy(v_œÄ)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Iteration vs Policy Iteration</h2></center>\n",
    "\n",
    "Policy Iteration often takes considerably fewer number of iterations to converge than Value Iteration.\n",
    "\n",
    "However, each policy iteration is more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo (MC) Methods</h2></center>\n",
    "\n",
    "Able to directly learn from experience (complete episodes) rather than relying on the prior knowledge of the environment dynamics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Mimics policy iteration - Given an experience following a policy to a terminal state, calculate the utility of that policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo (MC) Intuition</h2></center>\n",
    "\n",
    "Create a bunch of simulations of complete runs through the Environment.\n",
    "\n",
    "Each starting at random states in the Environment.\n",
    "\n",
    "Let the Agent move randomly until a termination state is achieved.\n",
    "\n",
    "For each episode, save 4 values: \n",
    "\n",
    "1. The initial state\n",
    "2. The actions taken\n",
    "3. The reward received \n",
    "4. The final state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The best policy is the sequence of empirical actions that gives us the highest reward on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo (MC) Demo</h2></center>\n",
    "\n",
    "<center><img src=\"https://mpatacchiola.github.io/blog/images/reinforcement_learning_model_free_monte_carlo_three_episodes_fast.gif\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo (MC) Demo Results</h2></center>\n",
    "\n",
    "<center><img src=\"https://mpatacchiola.github.io/blog/images/reinforcement_learning_model_free_monte_carlo_three_episodes_linear.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Compare DP vs. MC</h2></center>\n",
    "\n",
    "<center><img src=\"https://mpatacchiola.github.io/blog/images/reinforcement_learning_utility_estimation_dp_vs_mc.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The Agent starts in (0, 0) for both. There is a limited number of steps to illustrate limits.\n",
    "\n",
    "The utility estimations for two states are zero. \n",
    "\n",
    "This can be considered one of the limitations and at the same time one of the advantage of MC methods. \n",
    "\n",
    "The policy we are using, the transition probabilities, and the fact that the robot always start from the same position (bottom-left corner) are responsible of the wrong estimation for those states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " [Source](https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Temporal difference (TD) learning</h2></center>\n",
    "\n",
    "- Combination of Monte Carlo (MC) and dynamic programming (DP).\n",
    "- Like MC, Temporal difference (TD) learning learns directly from environment without a model.\n",
    "- Unlike MC, Temporal difference (TD) learning does not need all state-reward pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Like DP, TD use the expected values of next state to enrich the prediction and update estimates based on current state-reward pairs.\n",
    "\n",
    "- Thus, TD's approximation is slightly less precise and less sample-based. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "TD(0) is considered a bootstrapping method because it uses previous approximations, rather than completely using the entire trajectory‚Äôs reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>TD(0) is the simpilest</h2></center>\n",
    "<center> Learn at each and every step we take, thus also called one-step TD</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"images/td.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Value Estimation is at the heart of TD</h2></center>\n",
    "\n",
    "$$V(S_t) \\longleftarrow V(S) + Œ±[R_{t+1}+Œ≥V(S‚Ä≤)-V(S)]$$\n",
    "\n",
    "- Update the value function - $V(S_t)$   \n",
    "- Towards an estimated return - $R_{t+1}+Œ≥V(S‚Ä≤)$  \n",
    "- Œ± is the learning rate hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Compare the 3 Algorithms</h2></center> \n",
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/TD_MC_DP_backups.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " [Source](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#temporal-difference-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Given what we know about the 3 algorithms (DP, MC, and TD), what would be the space requirements for storing information for each?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Compare algorithms for space complexity</h2></center>\n",
    "<center><img src=\"images/compare.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Summary</h2></center>\n",
    "\n",
    "- Dynamic Programming (DP) is general method to solve specific kinds of divide-and-conquer problems.\n",
    "- Dynamic Programming (DP) is useful for finding the optimal policy in MDP by recursively searching the space for a policy maximizes the discounted cumulative reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Summary</h2></center>\n",
    "\n",
    "- Policy Iteration has 2 steps:\n",
    "    1. Policy Evaluation - For given policy, compute the estimated utility.\n",
    "    1. Policy Improvement - Pick a better policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Monte Carlo (MC) simulates possible policies with needing a model of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Temporal difference (TD) learning improves the value function by finding successfully better steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TD(Œª) (\"TD lambda\") algorithm to create TD-Gammon, a human-competitive Backgammon playing program. He wrote an article describing the approach, which you can find here.\n",
    "\n",
    "TD(Œª) was later extended to TDLeaf(Œª), specifically to better deal with Minimax searches. TDLeaf(Œª) has been used, for example, in the chess program KnightCap. You can read about TDLeaf in this paper.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "References:\n",
    "https://en.wikipedia.org/wiki/TD-Gammon\n",
    "\n",
    "http://www.research.ibm.com/massive/tdl.html\n",
    "\n",
    "https://arxiv.org/pdf/cs/9901001.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/vs.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/types.png\" width=\"75%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
