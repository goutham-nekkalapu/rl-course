{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Going-Deeper-into-Deep-Q-Learning\" data-toc-modified-id=\"Going-Deeper-into-Deep-Q-Learning-1\">Going Deeper into Deep Q-Learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#By-the-end-of-this-session,-you-should-be-able-to:\" data-toc-modified-id=\"By-the-end-of-this-session,-you-should-be-able-to:-2.0.1\">By the end of this session, you should be able to:</a></span></li></ul></li></ul></li><li><span><a href=\"#Experience-Replay-Review\" data-toc-modified-id=\"Experience-Replay-Review-3\">Experience Replay Review</a></span></li><li><span><a href=\"#Experience-Replay-Review\" data-toc-modified-id=\"Experience-Replay-Review-4\">Experience Replay Review</a></span></li><li><span><a href=\"#Experience-Replay-as-Regularization\" data-toc-modified-id=\"Experience-Replay-as-Regularization-5\">Experience Replay as Regularization</a></span></li><li><span><a href=\"#Experience-Replay-as-Regularization\" data-toc-modified-id=\"Experience-Replay-as-Regularization-6\">Experience Replay as Regularization</a></span></li><li><span><a href=\"#Stationary-vs.-Nonstationary-data\" data-toc-modified-id=\"Stationary-vs.-Nonstationary-data-7\">Stationary vs. Nonstationary data</a></span></li><li><span><a href=\"#-Nonstationary-Data-FTW-(literally-winning)\" data-toc-modified-id=\"-Nonstationary-Data-FTW-(literally-winning)-8\"> Nonstationary Data FTW (literally winning)</a></span></li><li><span><a href=\"#Actual-DQN-Results\" data-toc-modified-id=\"Actual-DQN-Results-9\">Actual DQN Results</a></span></li><li><span><a href=\"#-Experience-replay\" data-toc-modified-id=\"-Experience-replay-10\"> Experience replay</a></span></li><li><span><a href=\"#-Summary:-Experience-Replay-Advantages--over-Traditional-Q-learning\" data-toc-modified-id=\"-Summary:-Experience-Replay-Advantages--over-Traditional-Q-learning-11\"> Summary: <br>Experience Replay Advantages <br> over Traditional Q-learning</a></span></li><li><span><a href=\"#-Summary:-Experience-Replay-Advantages--over-Traditional-Q-learning\" data-toc-modified-id=\"-Summary:-Experience-Replay-Advantages--over-Traditional-Q-learning-12\"> Summary: <br>Experience Replay Advantages <br> over Traditional Q-learning</a></span></li><li><span><a href=\"#-The-Deadly-Triad-of-Q-learning\" data-toc-modified-id=\"-The-Deadly-Triad-of-Q-learning-13\"> The Deadly Triad of Q-learning</a></span></li><li><span><a href=\"#Sidebar---Off-policy-training\" data-toc-modified-id=\"Sidebar---Off-policy-training-14\">Sidebar - Off-policy training</a></span></li><li><span><a href=\"#-The-Deadly-Triad-of-Q-learning\" data-toc-modified-id=\"-The-Deadly-Triad-of-Q-learning-15\"> The Deadly Triad of Q-learning</a></span></li><li><span><a href=\"#Example-of-MDP-with-unbounded-estimates\" data-toc-modified-id=\"Example-of-MDP-with-unbounded-estimates-16\">Example of MDP with unbounded estimates</a></span></li><li><span><a href=\"#-Killing-The-Deadly-Triad-with-DQN\" data-toc-modified-id=\"-Killing-The-Deadly-Triad-with-DQN-17\"> Killing The Deadly Triad with DQN</a></span></li><li><span><a href=\"#Common-bugs-with-Deep-Q-Networks\" data-toc-modified-id=\"Common-bugs-with-Deep-Q-Networks-18\">Common bugs with Deep Q-Networks</a></span></li><li><span><a href=\"#Instability-in-DQN\" data-toc-modified-id=\"Instability-in-DQN-19\">Instability in DQN</a></span></li><li><span><a href=\"#Overweighted-Outliers\" data-toc-modified-id=\"Overweighted-Outliers-20\">Overweighted Outliers</a></span></li><li><span><a href=\"#Huber-Loss-Function\" data-toc-modified-id=\"Huber-Loss-Function-21\">Huber Loss Function</a></span></li><li><span><a href=\"#-Goldilocks-Neural-Network-Size\" data-toc-modified-id=\"-Goldilocks-Neural-Network-Size-22\"> Goldilocks Neural Network Size</a></span></li><li><span><a href=\"#Maximization-bias\" data-toc-modified-id=\"Maximization-bias-23\">Maximization bias</a></span></li><li><span><a href=\"#Grab-bag-of-helpful-techniques-for-DQN\" data-toc-modified-id=\"Grab-bag-of-helpful-techniques-for-DQN-24\">Grab-bag of helpful techniques for DQN</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-25\">Summary</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-26\">Summary</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-27\">Bonus Material</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Going Deeper into Deep Q-Learning</h2></center>\n",
    "\n",
    "<center><img src=\"images/dqn.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image Source](http://people.csail.mit.edu/hongzi/content/publications/DeepRM-HotNets16.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "#### By the end of this session, you should be able to:\n",
    "\n",
    "- Explain the advantages and disadvantages of Experience Replay.\n",
    "- List the Deadly Triad of Q-learning.\n",
    "- Explain how Deep Q-learning can mitigate that triad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Experience Replay Review</h2></center>\n",
    "\n",
    "Traditional Q-learning uses the \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ (~3 words) action to update Q-values.\n",
    "\n",
    "In contrast, Experience Replay uses a \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ (~4 words) actions to update Q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Experience Replay Review</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Traditional Q-learning uses __the single most recent action__ to update.\n",
    "\n",
    "In contrast, Experience Replay uses __a random sample of prior actions__ to update Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Experience Replay as Regularization</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "What is regularization?\n",
    "\n",
    "How does Experience Replay regularize Q-values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Experience Replay as Regularization</h2></center>\n",
    "\n",
    "Regularization is adding an additional constraint to a model to encourage generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Experience Replay regularizes Q-values by randomly sampling batches, then training the Deep Neural Network on those batches. \n",
    "\n",
    "Traditional Q-learning updates on __every single__ (s, a) pair which could lead to over-fitting the specific sequences of observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Stationary vs. Nonstationary data</h2></center>\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e1/Stationarycomparison.png/390px-Stationarycomparison.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Nonstationary Data FTW (literally winning)</h2></center>\n",
    "\n",
    "<center><img src=\"https://user-images.githubusercontent.com/46422351/53296086-14701500-3811-11e9-8281-6a9f513c7764.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Actual DQN Results</h2></center>\n",
    "\n",
    "<center><img src=\"https://jaromiru.com/media/dqn/basic_average_reward.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2> Experience replay</h2></center>\n",
    " \n",
    "Data distribution changes over time. As the Q-function gets better, you'll visit different (s , a , r, s′) transitions than earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Need to stabilize learning by mixing old and new transitions in the replay buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Otherwise \"catastrophic forgetting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Measuring Catastrophic Forgetting in Neural Networks](https://arxiv.org/abs/1708.02072) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Summary: <br>Experience Replay Advantages <br> over Traditional Q-learning</h2></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Improves data efficiency - Can sample with replacement from replay buffer. Traditional Q-learning throws away data after single use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Given sampling, experience replay can minimize correlations in the observation sequences. Experience replay learns more generalized properties of the state space, less likely to memorize specific sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Summary: <br>Experience Replay Advantages <br> over Traditional Q-learning</h2></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Smooths over changes in the data distribution - another variation of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " [Source](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deep-q-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> The Deadly Triad of Q-learning</h2></center>\n",
    "\n",
    "1. Function approximation\n",
    "\n",
    "1. Bootstrapping \n",
    "\n",
    "1. Off-policy training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Sidebar - Off-policy training</h2></center>\n",
    "\n",
    "- Behavior Policy - The policy that yields the current action.\n",
    "\n",
    "- Target Policy - The policy that is being learned / updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- On-policy training - Target Policy = Behavior Policy. Update only the Behavior Policy (sequential)\n",
    "\n",
    "- Off-policy training - Update all relevant Target Policies (parallel). Allow agent to learn policies not acted upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "1. __Function approximation__ A powerful, scalable way of generalizing from a state space much larger than the memory and computational resources (e.g., linear function approximation or ANNs).\n",
    "\n",
    "1. __Bootstrapping__ Update targets that include existing estimates rather than relying exclusively on actual rewards and complete returns (as in MC methods).\n",
    "\n",
    "\n",
    "1. __Off-policy training__ Training on a distribution of transitions other than that produced by the target policy. Sweeping through the state space and updating all states uniformly, as in dynamic programming, does not respect the target policy and is an example of off-policy training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> The Deadly Triad of Q-learning</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Learning can diverge.\n",
    "\n",
    "Value estimates becoming unbounded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Example of MDP with unbounded estimates</h2></center>\n",
    "\n",
    "<center><img src=\"images/diverge.png\" width=\"75%\"/></center>\n",
    "\n",
    "The system has no rewards, therefore $w_* = 0$\n",
    "\n",
    "Off-policy learning will diverge, aka move away from true estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Killing The Deadly Triad with DQN</h2></center>\n",
    " \n",
    "Using DQN can stabilize the training:\n",
    "\n",
    "- Experience Replay encourages longer multi-step returns, thus less likely to diverge.\n",
    "- DQN have increased capacity. Larger, more flexible networks diverge less easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use Double Deep Q-learning to correct for overestimation bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sources:\n",
    "\n",
    "- Sutton & Barton p 264\n",
    "- [Deep Reinforcement Learning and the Deadly Triad](https://arxiv.org/pdf/1812.02648.pdf) from DeepMind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Common bugs with Deep Q-Networks</h2></center>\n",
    "\n",
    "- Instability\n",
    "- Overweighted outliers\n",
    "- Incorrect neural network size\n",
    "- Maximization bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Instability in DQN</h2></center>\n",
    " \n",
    "Problem - Each update is large. Parameters values change wildly.\n",
    "\n",
    "What are possible solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Increase batch size.\n",
    "- Increase the amount of older data in replay buffer.\n",
    "- Decrease either Q-learning rate or SGD learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Overweighted Outliers</h2></center>\n",
    "\n",
    "Remember a MSE loss function:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\Sigma_{i=1}^{n}{({y_i -\\hat{y_i}}^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could switch loss functions to clip or attenuate extreme updates.\n",
    "\n",
    "What are other possible loss functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://i.stack.imgur.com/vXMgz.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Huber Loss Function</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"images/huber.png\" width=\"75%\"/></center>\n",
    "<br>\n",
    "The parameter $t_H$ defines a threshold (based on the distance between target and prediction) that makes the loss function switch from a squared error to an absolute one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " [Source](https://learning.oreilly.com/library/view/machine-learning-algorithms/9781789347999/7159b3a9-c481-4227-9641-0b0b59cac4ff.xhtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Goldilocks Neural Network Size</h2></center>\n",
    "\n",
    "If network is too small, it may fail to approximate the Q function properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If network is too big, it may overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Maximization bias</h2></center>\n",
    "\n",
    "$$Q(s, a) = r_0 + \\gamma \\max_a Q^*(s', a)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $max$ function is trouble:\n",
    "\n",
    "1. Non-linear \n",
    "1. The model is bias towards overestimation of the Q function’s value, thus leads to poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Double Q-learning can help to mitigate maximization bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grab-bag of helpful techniques for DQN</h2></center>\n",
    "\n",
    "- Prioritized experience replay - Remember all your clever ways to resample simulations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Utilizing known truth - If there is ground-truth $ Q(s,a)=r$, anchor those in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, in video games there is clear outcome for certain states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Summary</h2></center>\n",
    "\n",
    "- Experience Replay can stabilize and generalize Q-learning by using more and better examples for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Q-learning can be unbounded because of the Deadly Triad:\n",
    "    1. Function approximation\n",
    "    1. Bootstrapping \n",
    "    1. Off-policy training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Summary</h2></center>\n",
    "\n",
    "- Deep Q-learning is a good approach because:\n",
    "    - Deep neural networks can do function approximation well.\n",
    "    - Bootstrapping is improved through Experience Replay.\n",
    "    - __Double__ Deep Q-learning can better estimate off-policy values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://jaromiru.com/media/dqn/dqn.png\" width=\"45%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image source](https://jaromiru.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
